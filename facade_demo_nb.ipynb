{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FACADE: Fake Article Classification and Decision Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is meant to run **FACADE** system locally.\n",
    "\n",
    "After installing the libraries listed in *requirements.txt*, just run the entire notebook and have fun! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\purifica\\AppData\\Roaming\\Python\\Python38\\site-packages\\huggingface_hub\\snapshot_download.py:6: FutureWarning: snapshot_download.py has been made private and will no longer be available from version 0.11. Please use `from huggingface_hub import snapshot_download` to import the only public function in this module. Other members of the file may be changed without a deprecation notice.\n",
      "  warnings.warn(\n",
      "\n",
      "The dash_html_components package is deprecated. Please replace\n",
      "`import dash_html_components as html` with `from dash import html`\n",
      "\n",
      "The dash_core_components package is deprecated. Please replace\n",
      "`import dash_core_components as dcc` with `from dash import dcc`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\purifica\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\purifica\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\purifica\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\purifica\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "from urllib.error import HTTPError\n",
    "from boilerpy3 import extractors\n",
    "import validators\n",
    "import re\n",
    "import contractions\n",
    "import unicodedata\n",
    "import ftfy\n",
    "import secrets\n",
    "import threading\n",
    "import random\n",
    "import nltk\n",
    "from nltk import ne_chunk, pos_tag\n",
    "from nltk.tree import Tree\n",
    "from nltk.tokenize import RegexpTokenizer, sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.wsd import lesk\n",
    "import language_tool_python\n",
    "import textstat\n",
    "from textblob import TextBlob\n",
    "import string\n",
    "import concurrent.futures\n",
    "import pickle\n",
    "from collections import Counter, defaultdict\n",
    "from operator import itemgetter\n",
    "import itertools\n",
    "import tensorflow as tf\n",
    "import spacy\n",
    "from sentence_transformers import util, SentenceTransformer\n",
    "from transformers import pipeline\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "import shap\n",
    "import html\n",
    "import dash_bootstrap_components as dbc\n",
    "from explainerdashboard import ClassifierExplainer, ExplainerDashboard\n",
    "from flask import Flask, request, render_template, jsonify\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('brown')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define helper classes and variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FileLocker:\n",
    "    def __init__(self):\n",
    "        self.lock = threading.Lock()\n",
    "\n",
    "    def acquire_file_lock(self, file):\n",
    "        while os.path.exists(file+\".lock\"):\n",
    "            time.sleep(0.05)\n",
    "        with self.lock:\n",
    "            with open(file+\".lock\", \"w\") as f:\n",
    "                f.write(\"locked\")\n",
    "        return True\n",
    "\n",
    "    def delete_file_lock(self, file):\n",
    "        with self.lock:\n",
    "            if os.path.exists(file+\".lock\"):\n",
    "                os.remove(file+\".lock\")\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_locker = FileLocker()\n",
    "stored_requests = dict()\n",
    "running_db = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load standard classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english)\n",
      "No sentence-transformers model found with name C:\\Users\\purifica/.cache\\torch\\sentence_transformers\\microsoft_deberta-v2-xlarge-mnli. Creating a new one with MEAN pooling.\n",
      "Some weights of the model checkpoint at C:\\Users\\purifica/.cache\\torch\\sentence_transformers\\microsoft_deberta-v2-xlarge-mnli were not used when initializing DebertaV2Model: ['classifier.bias', 'classifier.weight', 'pooler.dense.weight', 'pooler.dense.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "Entailment_Classifier = pipeline(\"text-classification\", model = \"roberta-large-mnli\")\n",
    "Sentiment_Classifier = pipeline(\"sentiment-analysis\")\n",
    "zero_classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "attr_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "attr_model.max_seq_length = 512\n",
    "dberta_model = SentenceTransformer('microsoft/deberta-v2-xlarge-mnli')\n",
    "model_for_personality = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load pickle files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "PICKLE_FILES_FOLDER = \"pickle_files\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle files PIPELINE-1\n",
    "with open(os.path.join(PICKLE_FILES_FOLDER, 'model_final_ll.pickle'), 'rb') as handle:\n",
    "        model_ll = pickle.load(handle)\n",
    "\n",
    "with open(os.path.join(PICKLE_FILES_FOLDER, 'tf_idf_with_words_ll.pickle'), 'rb') as handle:\n",
    "        tf_idf_vectorizer_ll = pickle.load(handle)\n",
    "\n",
    "# Pickle files PIPELINE-2\n",
    "with open(os.path.join(PICKLE_FILES_FOLDER, 'tfidf_vect_ngram_personality.pickle'), 'rb') as handle:\n",
    "        vec_for_personality = pickle.load(handle)\n",
    "        \n",
    "with open(os.path.join(PICKLE_FILES_FOLDER, 'model_topics.pickle'), 'rb') as handle:\n",
    "        model_for_topic = pickle.load(handle)\n",
    "        \n",
    "with open(os.path.join(PICKLE_FILES_FOLDER, 'adjective_embeddings_labels.pickle'), 'rb') as handle:\n",
    "        models_for_adj = pickle.load(handle)\n",
    "        \n",
    "with open(os.path.join(PICKLE_FILES_FOLDER, 'glove_dict.pickle'), 'rb') as handle:\n",
    "        glove_embeddings = pickle.load(handle)\n",
    "\n",
    "with open(os.path.join(PICKLE_FILES_FOLDER, 'df_Real_Markov_Analysis_trained.pickle'), 'rb') as handle:\n",
    "        trained_markov_real_structure = pickle.load(handle)\n",
    "        \n",
    "with open(os.path.join(PICKLE_FILES_FOLDER, 'df_Fake_Markov_Analysis_trained.pickle'), 'rb') as handle:\n",
    "        trained_markov_fake_structure = pickle.load(handle)\n",
    "        \n",
    "with open(os.path.join(PICKLE_FILES_FOLDER, 'semantic_attribution_embeddings_trained.pickle'), 'rb') as handle:\n",
    "        trained_dberta_embeddings = pickle.load(handle)\n",
    "        \n",
    "with open(os.path.join(PICKLE_FILES_FOLDER, 'data_train.pickle'), 'rb') as handle:\n",
    "        trained_data = pickle.load(handle)\n",
    "        \n",
    "with open(os.path.join(PICKLE_FILES_FOLDER, 'Mapping_Sentences_doc2vec_new.pickle'), 'rb') as handle:\n",
    "        mapping_sent_doc = pickle.load(handle)       \n",
    "        \n",
    "with open(os.path.join(PICKLE_FILES_FOLDER, 'model_doc2vec.pickle'), 'rb') as handle:\n",
    "        doc2vec_model = pickle.load(handle)\n",
    "        \n",
    "with open(os.path.join(PICKLE_FILES_FOLDER, 'all_columns_seq_high_level_wo_tone.pickle'), 'rb') as handle:\n",
    "        seq_of_col = pickle.load(handle)\n",
    "        \n",
    "with open(os.path.join(PICKLE_FILES_FOLDER, 'model_highlevel_wo_tone.pickle'), 'rb') as handle:\n",
    "        model_hl = pickle.load(handle)\n",
    "\n",
    "with open(os.path.join(PICKLE_FILES_FOLDER, 'all_columns_seq_high_level_wo_zero.pickle'), 'rb') as handle:\n",
    "        seq_of_col_wo_zeroshot = pickle.load(handle)\n",
    "        \n",
    "with open(os.path.join(PICKLE_FILES_FOLDER, 'model_highlevel_wo_zeroshot.pickle'), 'rb') as handle:\n",
    "        model_hl_wo_zeroshot = pickle.load(handle)\n",
    "\n",
    "# Pickle files EXPLAINABILITY\n",
    "with open(os.path.join(PICKLE_FILES_FOLDER, 'model_highlevel_reduced_UI_use_explain.pickle'), 'rb') as handle:\n",
    "        model_explain = pickle.load(handle)\n",
    "\n",
    "with open(os.path.join(PICKLE_FILES_FOLDER, 'all_sentences_training_for_explain.pickle'), 'rb') as handle:\n",
    "        all_sentences_training = pickle.load(handle)\n",
    "\n",
    "with open(os.path.join(PICKLE_FILES_FOLDER, 'col_keep_for_explaination.pickle'), 'rb') as handle:\n",
    "        col_explain = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "explain_dict_ll = {\n",
    "    'length_body' : ('Length of Main Text','The length of the main body text (i.e. number of words in the news).'),\n",
    "    'Type_Token_body' : ('TTR of Main Text','The type token ratio (number of unique words compared to all words used) of the main body text.'),\n",
    "    'average_wl_body' : ('Avg. Word Length of Main Text','The average word length inside the main body text.'),\n",
    "    'length_headline' : ('Length of Headline','The length of the title of the news in number of words.'),\n",
    "    'Type_Token_headlines' : ('TTR of Headline','The type token ratio (number of unique words compared to all words used) of the title of the news.'),\n",
    "    'average_wl_headline' : ('Avg. Word Length of Headline','The average word length inside the title of the news.'),\n",
    "    'sentence_count' : ('No. of Sentences','The total of sentences of the news.'),\n",
    "    'Avg_Words_Sentence' : ('Avg. Words per Sentence','The average number of words per sentence.'),\n",
    "    'count_names' : ('No. of Names','The total amount of person names inside the news article.'),\n",
    "    'count_grammar_mistakes' : ('No. of Grammar Mistakes','The amount of grammatical mistakes inside the text.'),\n",
    "    'ease_reading_score' : ('Reading Score','The reading score (denoting how easy a text is to read) for the text.'),\n",
    "    'num_pronouns' : ('No. of Pronouns','The number of pronouns used in the text.'),\n",
    "    'num_poss_pronouns' : ('No. of Poss. Pronouns','The number of possesive pronouns used in the text.'),\n",
    "    'num_noun_phrases' : ('No. of Noun Phrases','The number of noun phrases (e.g. the news) used in the text.'),\n",
    "    'pol' : ('Polarity','The polarity of the text, i.e. how positive or negative the text is written.'),\n",
    "    'sub' : ('Subjectivity','The subjectivity of the text, i.e. how many opinionated words are used.'),\n",
    "    'NV' : ('Ratio of Noun-Verb','The ratio of nouns compared to verbs (nouns/verbs).'),\n",
    "    'NA' : ('Ratio of Noun-Adjective','The ratio of nouns compared to adjectives (nouns/adjectives).'),\n",
    "    'VN' : ('Ratio of Verb-Noun','The ratio of verbs to nouns (verbs/nouns).'),\n",
    "    'VA' : ('Ratio of Verb-Adjective','The ratio of verbs to adjectives (verbs/adjectives).'),\n",
    "    'AN' : ('Ratio of Adjective-Noun','The ratio of adjectives to nouns (adjectives/nouns).'),\n",
    "    'AV' : ('Ratio of Adjective-Verb','The ratio of adjectives to verbs (adjectives/verbs).'),\n",
    "    'num_digits' : ('No. of Digits','How many digits are found inside the text.'),\n",
    "    'stop_words' : ('No. of Stopwords','How many stopwords are found inside the text. Stopwords are words, that do not contain meaningful information, e.g. words like of, the, in, etc.'),\n",
    "    'upper_case_words' : ('No. of Capitalized Words','The count of words that are capitalized inside the text.'),\n",
    "    'num_puntuation' : ('No. of Punctuation','The amount of punctuation used in the text.'),\n",
    "    'num_char' : ('No. of Characters','The amount of characters the text is made of.')\n",
    "}\n",
    "\n",
    "explain_dict_hl = {\n",
    "    'Real_weight' : ('Ratio of Real Sentences', 'The ratio of sentences, that are attributed as being real, compared to fake sentences (attributed to being wrong).'),\n",
    "    'markov_pred' : ('Markov Prediction', 'Whether the markov model predicted the structure of the news article to be real or fake.'),\n",
    "    'End_Real' : ('Ended on Real Sentence', 'Whether the last sentence in the news article was attributed to being real.'),\n",
    "    'INTERNATIONAL POLITICAL_Norm' : ('International Politics Topic', 'The normalized (over all topics) value for the appearance of the topic \"International Politics\" inside the news article.')\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check and handle old requests\n",
    "def check_for_old_requests():\n",
    "    global stored_requests\n",
    "    # Throw out requests, that are too old\n",
    "    to_throw = []\n",
    "    for key in stored_requests.keys():\n",
    "        if time.time() - stored_requests[key][\"time\"] > 600:\n",
    "            to_throw.append(key)\n",
    "    for key in to_throw:\n",
    "        del stored_requests[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read content from URL\n",
    "def read_content_from_url(news_url):\n",
    "    try:\n",
    "        extractor = extractors.ArticleExtractor()\n",
    "        doc = extractor.get_doc_from_url(news_url)\n",
    "    except HTTPError :\n",
    "        url = news_url\n",
    "        extractor = extractors.ArticleExtractor()\n",
    "        resp = requests.get(url)\n",
    "        if resp.ok:\n",
    "            complete_text = extractor.get_content(resp.text)\n",
    "            content = complete_text.partition('\\n')[2]\n",
    "            title = complete_text.partition('\\n')[0]\n",
    "        else:\n",
    "            raise Exception(f'Failed to get URL: {resp.status_code}')\n",
    "    else:\n",
    "        content = doc.content\n",
    "        title = doc.title\n",
    "        title=title.split('|')[0]\n",
    "\n",
    "    return content,title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "def prepare_data(text, headline = None):\n",
    "    \n",
    "    if validators.url(text) == True:\n",
    "        data, title = read_content_from_url(text)\n",
    "    else:\n",
    "        data = text\n",
    "        title = headline\n",
    "    \n",
    "    data = re.sub('\\S*@\\S*\\s?', '', data)\n",
    "    data = re.sub('\\s+', ' ', data)\n",
    "    data = re.sub(\"\\n\", \"\", data)\n",
    "    data = contractions.fix(data)\n",
    "    data = data.strip()\n",
    "    data = re.sub(\"'\", \"\", data)\n",
    "    data = re.sub(r'you\\.(?=[a-zA-Z0-9])', 'U.', data)\n",
    "    data = re.sub(r'https\\S+', '', data)\n",
    "    data = re.sub(r'http\\S+', '', data)\n",
    "    data = re.sub(r'www\\S+', '', data)\n",
    "    data = unicodedata.normalize('NFKD', ftfy.fix_text(data)).encode('ascii', 'ignore').decode(\"utf-8\")\n",
    "    title = re.sub('\\S*@\\S*\\s?', '', title)\n",
    "    title = re.sub('\\s+', ' ', title)\n",
    "    title = re.sub(\"\\n\", \"\", title)\n",
    "    title = contractions.fix(title)\n",
    "    title = title.strip()\n",
    "    title = re.sub(\"'\", \"\", title)\n",
    "    title = re.sub(r'you\\.(?=[a-zA-Z0-9])', 'U.', title)\n",
    "    title = re.sub(r'https\\S+', '', title)\n",
    "    title = re.sub(r'http\\S+', '', title)\n",
    "    title = re.sub(r'www\\S+', '', title)\n",
    "    title = unicodedata.normalize('NFKD', ftfy.fix_text(title)).encode('ascii', 'ignore').decode(\"utf-8\")\n",
    "    return [data,title]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide\n",
    "def divide(num1, num2):\n",
    "    if num2 == 0:\n",
    "        result_list = 0\n",
    "    else:\n",
    "        result_list = num1/num2\n",
    "    return result_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IE Preprocess\n",
    "def ie_preprocess(document):\n",
    "    stop = stopwords.words('english')\n",
    "    document = ' '.join([i for i in document.split() if i not in stop])\n",
    "    sentences = sent_tokenize(document)\n",
    "    sentences = [word_tokenize(sent) for sent in sentences]\n",
    "    sentences = [pos_tag(sent) for sent in sentences]\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract names\n",
    "def extract_names(document):\n",
    "    names = []\n",
    "    sentences = ie_preprocess(document)\n",
    "    for tagged_sentence in sentences:\n",
    "        for chunk in ne_chunk(tagged_sentence):\n",
    "            if type(chunk) == Tree:\n",
    "                if chunk.label() == 'PERSON':\n",
    "                    names.append(' '.join([c[0] for c in chunk]))\n",
    "    return names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Low-level features extraction\n",
    "def low_level_features(text, vec, headline=None):\n",
    "    data, title = prepare_data(text, headline)\n",
    "\n",
    "    tool = language_tool_python.LanguageTool('en-US')\n",
    "    tokenizer = RegexpTokenizer(r'[^\\W’]+')\n",
    "    words = tokenizer.tokenize(data)\n",
    "    length_body = len(words)\n",
    "    unique_length_body = len(set(words))\n",
    "    average_wl_body = sum(len(word) for word in words) / len(words)\n",
    "    Type_Token_body = unique_length_body/length_body\n",
    "    words_headline = tokenizer.tokenize(title)\n",
    "    length_headline = len(words_headline)\n",
    "    unique_length_headline = len(set(words_headline))\n",
    "    average_wl_headline = sum(len(word) for word in words_headline) / len(words_headline)\n",
    "    Type_Token_headlines = unique_length_headline/length_headline\n",
    "    sentences = sent_tokenize(data)\n",
    "    sentences = [elem for elem in sentences if len(re.findall(r'\\w+', elem)) >1]\n",
    "    sentence_count = len(sentences)\n",
    "    Avg_Words_Sentence = divide(length_body,sentence_count)\n",
    "    names_list = extract_names(data)\n",
    "    count_names = len(names_list)\n",
    "    grammar = tool.check(data)\n",
    "    count_grammar_mistakes = len(grammar)\n",
    "    ease_reading_score = textstat.text_standard(data, float_output=True)\n",
    "    blob = TextBlob(data)\n",
    "    num_pronouns = len([w for (w, pos) in blob.pos_tags if pos == 'PRP'])\n",
    "    num_poss_pronouns = len([w for (w, pos) in blob.pos_tags if pos == 'PRP$'])\n",
    "    num_noun_phrases = len(blob.noun_phrases)\n",
    "    num_nouns = len([w for (w, pos) in blob.pos_tags if pos[0] == 'N'])\n",
    "    num_verbs = len([w for (w, pos) in blob.pos_tags if pos[0] == 'V'])\n",
    "    num_adj = len([w for (w, pos) in blob.pos_tags if pos[0] == 'J'])\n",
    "    pol = blob.sentiment[0]\n",
    "    sub = blob.sentiment[1]\n",
    "    NV = divide(num_nouns,num_verbs)\n",
    "    NA = divide(num_nouns,num_adj)\n",
    "    VN = divide(num_verbs,num_nouns)\n",
    "    VA = divide(num_verbs,num_adj)\n",
    "    AN = divide(num_adj,num_nouns)\n",
    "    AV = divide(num_adj,num_verbs)\n",
    "    stop = stopwords.words('english')\n",
    "    num_digits = len([x for x in tokenizer.tokenize(data) if x.isdigit()])\n",
    "    stop_words = len([x for x in tokenizer.tokenize(data) if x in stop])\n",
    "    upper_case_words = len([x for x in tokenizer.tokenize(data) if x.isupper()])\n",
    "    num_puntuation = len(\"\".join(_ for _ in data if _ in string.punctuation))\n",
    "    num_char = len(data)\n",
    "    tf_idf_features = np.array(vec.transform([data]).todense())[0]\n",
    "    tf_idf_features = pd.DataFrame(tf_idf_features).T\n",
    "    tf_idf_features.columns = ['212', 'video', 'funeral', 'al', 'museum', 'family', 'terrorists', 'died', 'sister', 'post', 'play', 'devoted', 'en', 'league', 'trump','company', 'please', 'earth', 'daughter', 'coach', 'gun', 'music', 'graduated', 'theater', 'survived', '2017', 'husband', 'mother', 'street', 'gold', 'art', 'la', 'loving', 'grandchildren', 'talk', 'avenue', 'father', 'percent', 'wife', 'et', 'game', 'season', 'de', 'beloved']\n",
    "    low_level = pd.DataFrame([length_body, Type_Token_body, average_wl_body, length_headline,Type_Token_headlines, average_wl_headline, sentence_count, Avg_Words_Sentence,count_names, count_grammar_mistakes, ease_reading_score, num_pronouns, num_poss_pronouns, num_noun_phrases, pol, sub, NV, NA, VN, VA, AN, AV, num_digits, stop_words, upper_case_words, num_puntuation, num_char]).T\n",
    "    low_level.columns = ['length_body', 'Type_Token_body', 'average_wl_body', 'length_headline', 'Type_Token_headlines', 'average_wl_headline', 'sentence_count', 'Avg_Words_Sentence', 'count_names', 'count_grammar_mistakes', 'ease_reading_score', 'num_pronouns', 'num_poss_pronouns', 'num_noun_phrases', 'pol', 'sub', 'NV', 'NA', 'VN', 'VA', 'AN', 'AV', 'num_digits', 'stop_words', 'upper_case_words', 'num_puntuation', 'num_char']\n",
    "    df_low_level = pd.concat([low_level, tf_idf_features], axis=1)\n",
    "    \n",
    "    return (df_low_level, data, title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare sentences for Pipeline 2\n",
    "def prepare_sentences_pipeline2(text):\n",
    "    sentences_temp =  sent_tokenize(text)\n",
    "    sentences = [elem for elem in sentences_temp if len(re.findall(r'\\w+', elem)) >1]\n",
    "    sentence_pairs = [' '.join(sentences[i:i+2]) for i in range(len(sentences))]\n",
    "    sentence_pairs.remove(sentence_pairs[-1])\n",
    "    return [sentences, sentence_pairs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk sentences\n",
    "def chunk_sentences(sentences):\n",
    "    chunked_sentences=[]\n",
    "    length = len(sentences)\n",
    "    if length == 1:\n",
    "        chunk_size = 1\n",
    "    else:\n",
    "        chunk_size = round(length/3)\n",
    "    for j in range(0,len(sentences),chunk_size):\n",
    "        chunked_sentences.append(' '.join(sentences[j:j+chunk_size]))\n",
    "        \n",
    "    if len(chunked_sentences) > 3:\n",
    "        chunked_sentences[2] = ' '.join(chunked_sentences[2:])\n",
    "        del chunked_sentences[3:]\n",
    "        \n",
    "    return(chunked_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Entailment features\n",
    "def entailment_features(sentence_pairs, sentences):\n",
    "    labels = ['CONTRADICTION', 'ENTAILMENT', 'NEUTRAL']\n",
    "    if len(sentence_pairs)!=0:\n",
    "        temp1 = [Entailment_Classifier(elem, truncation=True)[0]['label'] for elem in sentence_pairs]\n",
    "    else:\n",
    "        temp1 = [Entailment_Classifier(elem, truncation=True)[0]['label'] for elem in sentences]\n",
    "    counter = Counter(temp1)\n",
    "    total = sum(counter.values())\n",
    "    for item, count in counter.items():\n",
    "        counter[item] /= total\n",
    "        temp1_counter = (sorted(counter.items()))\n",
    "    elem=[j[0] for j in temp1_counter]\n",
    "    if labels not in elem:\n",
    "        non_intersection = set(labels) - set(elem)\n",
    "    for item in non_intersection:\n",
    "            temp1_counter.append((item, 0))\n",
    "\n",
    "    df_entailment_type1 = pd.DataFrame({x[0]:x[1:] for x in temp1_counter})\n",
    "    \n",
    "    if len(sentence_pairs) != 0:\n",
    "        temp2 = [Entailment_Classifier(elem, truncation=True)[0] for elem in sentence_pairs]\n",
    "    else:\n",
    "        temp2 = [Entailment_Classifier(elem, truncation=True)[0] for elem in sentences]\n",
    "        \n",
    "    temp2_counter = []\n",
    "    sorted_list = sorted(temp2, key=itemgetter('label'))\n",
    "    for key, group in itertools.groupby(sorted_list, lambda item: item[\"label\"]):\n",
    "                temp2_counter.append((key, np.mean([item[\"score\"] for item in group])))\n",
    "    \n",
    "    elem = [j[0] for j in temp2_counter]\n",
    "    if labels not in elem:\n",
    "        non_intersection = set(labels) - set(elem)\n",
    "    for item in non_intersection:\n",
    "            temp2_counter.append((item,0))\n",
    "    df_entailment_type2 = pd.DataFrame({x[0]:x[1:] for x in temp2_counter})\n",
    "    \n",
    "    return[temp1, df_entailment_type1, df_entailment_type2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Personality feature\n",
    "def personality_feature(text, vec_for_personality, model_for_personality):\n",
    "    model_for_personality = tf.keras.models.load_model(os.path.join(PICKLE_FILES_FOLDER, \"personality_model\"))\n",
    "    personality_temp= np.array(vec_for_personality.transform([text]).todense())\n",
    "    predictions= model_for_personality.predict(personality_temp.reshape(1,-1)).round()[0]\n",
    "    df_personality = pd.DataFrame(predictions).T\n",
    "    df_personality.columns = ['Extrovert', 'Sensing', 'Feeling', 'Perceiving']\n",
    "    return (df_personality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Sentiment feature\n",
    "def sentiment_feature(sentences):\n",
    "    labels = ['POSITIVE','NEGATIVE']\n",
    "    temp1 = [Sentiment_Classifier(elem, truncation=True)[0]['label'] for elem in sentences]\n",
    "    counter = Counter(temp1)\n",
    "    total = sum(counter.values())\n",
    "    for item, count in counter.items():\n",
    "        counter[item] /= total\n",
    "        temp1_counter = (sorted(counter.items()))\n",
    "    elem=[j[0] for j in temp1_counter]\n",
    "    if labels not in elem:\n",
    "        non_intersection = set(labels) - set(elem)\n",
    "    for item in non_intersection:\n",
    "            temp1_counter.append((item, 0))\n",
    "\n",
    "    df_sentiment_type1 = pd.DataFrame({x[0]:x[1:] for x in temp1_counter})\n",
    "    \n",
    "    temp2 = [Sentiment_Classifier(elem,truncation=True)[0] for elem in sentences]\n",
    "    temp2_counter = []\n",
    "    sorted_list = sorted(temp2, key=itemgetter('label'))\n",
    "    for key, group in itertools.groupby(sorted_list, lambda item: item[\"label\"]):\n",
    "        temp2_counter.append((key, np.mean([item[\"score\"] for item in group])))\n",
    "    \n",
    "    elem=[j[0] for j in temp2_counter]\n",
    "    if labels not in elem:\n",
    "        non_intersection = set(labels) - set(elem)\n",
    "    for item in non_intersection:\n",
    "            temp2_counter.append((item, 0))\n",
    "    df_sentiment_type2 = pd.DataFrame({x[0]:x[1:] for x in temp2_counter})\n",
    "    \n",
    "    return [df_sentiment_type1, df_sentiment_type2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Topic features\n",
    "def topic_feature(text, sentences, dberta_model, model_for_topic):\n",
    "    dberta_embedding = dberta_model.encode(text)\n",
    "    topic_raw = model_for_topic.predict(dberta_embedding.reshape(1,-1))[0]\n",
    "    if np.sum(topic_raw) != 0:\n",
    "        topic_embedding = np.array(topic_raw) / np.sum(np.array(topic_raw))\n",
    "    else:\n",
    "        topic_embedding = topic_raw\n",
    "    \n",
    "    dberta_embedding_sentence = dberta_model.encode(sentences)\n",
    "    temp = [model_for_topic.decision_function(dberta_embedding_sentence)][0]\n",
    "    topic_sentence = np.argmax(temp, axis=1)\n",
    "    return [topic_raw, topic_embedding, topic_sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Zero-shot learning topics feature\n",
    "def zero_shot_learning_topic(text):\n",
    "    sequence_to_classify = text\n",
    "    candidate_labels = ['war', 'government', 'politics', 'education', 'health', 'environment', 'economy', 'business', 'entertainment','sports']\n",
    "    result = zero_classifier(sequence_to_classify, candidate_labels)\n",
    "    temp = pd.DataFrame({'labels': result['labels'], 'score': result['scores']})\n",
    "    temp = temp.sort_values(by=['labels']).reset_index(drop=True)\n",
    "    headers = ['business', 'economy', 'education', 'entertainment', 'environment', 'government', 'health', 'politics', 'sports', 'war']\n",
    "    df_doc_zero_topics = pd.DataFrame(temp.T.values[1:], columns=headers)\n",
    "    \n",
    "    sentences, sentence_pairs = prepare_sentences_pipeline2(text)\n",
    "    \n",
    "    sequence_to_classify = sentences\n",
    "    candidate_labels = ['war', 'government', 'politics', 'education', 'health', 'environment', 'economy', 'business', 'entertainment', 'sports']\n",
    "    result = (zero_classifier(sequence_to_classify, candidate_labels))\n",
    "    result = [list(elem.values())[1:] for elem in result ] \n",
    "    result = [item[0] for elem in result for item in elem]\n",
    "    zero_shot_topic_sentences = [result[i] for i in range(len(result)) if i % 2 == 0]\n",
    "    \n",
    "    return[df_doc_zero_topics, zero_shot_topic_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute POS segment\n",
    "def pos_segment(sentences):\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    pos_segments_spacy=[]\n",
    "    chunks = chunk_sentences(sentences)\n",
    "    for i in range(len(chunks)):\n",
    "        pos_segments_spacy.append([])\n",
    "        doc = nlp(chunks[i])\n",
    "        for token in doc:\n",
    "            pos_segments_spacy[i].append(tuple((token.text, token.pos_)))\n",
    "    return(pos_segments_spacy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute custom adjective embeddings\n",
    "def custom_adjective_embeddings(sentences, models_adj, glove_embeddings):\n",
    "    adj_topic_list = [i for i in range(0,8)]\n",
    "    segments_A = []\n",
    "    pos_segments = pos_segment(sentences)\n",
    "    for i in range(len(pos_segments)):\n",
    "        segments_A.append([])\n",
    "        for k in range(len(pos_segments[i])):\n",
    "            if pos_segments[i][k][1] in ['ADJ']:\n",
    "                segments_A[i].append(pos_segments[i][k][0])\n",
    "                \n",
    "    categories_Adj = []\n",
    "    for i in range(len(segments_A)):\n",
    "        for k in range(len(segments_A[i])):\n",
    "                temp = wordnet.synsets(segments_A[i][k], pos=wordnet.ADJ)\n",
    "                if temp:\n",
    "                    temp_2 = temp[0].lemmas()[0].key().split(\"%\",1)[1].split(\":\")[0]\n",
    "                    if temp_2 == '5':\n",
    "                        categories_Adj.append(((temp[0].lemmas()[0].key().split(':', 4)[3]), (temp[0].lemmas()[0].key().split('%', 4)[0])))              \n",
    "\n",
    "    embeddings = [glove_embeddings.get(elem[0]) if elem[0] in (glove_embeddings) else glove_embeddings.get(elem[1]) if elem[1] in (glove_embeddings) else 'None' for elem in categories_Adj]\n",
    "    embeddings = list(filter((\"None\").__ne__,embeddings))\n",
    "    if len(embeddings) == 0:\n",
    "        topic_embedding_adj = np.zeros(len(adj_topic_list))\n",
    "    else:\n",
    "        labels_adj = models_adj.predict(embeddings)\n",
    "\n",
    "        topic_embedding_adj = np.zeros(len(adj_topic_list))\n",
    "        num_seg = len(labels_adj) \n",
    "        for i in range(num_seg):\n",
    "            if num_seg == 0:\n",
    "                topic_embedding_adj=np.zeros(len(adj_topic_list))\n",
    "            else:\n",
    "                index = labels_adj[i]\n",
    "                topic_embedding_adj[index] += 1/num_seg\n",
    "    \n",
    "    return(topic_embedding_adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute custom verb embeddings\n",
    "def custom_verb_embeddings(sentences):\n",
    "    verb_topic_list = [str(i)for i in range(29,44)]\n",
    "    segments_V = []\n",
    "    pos_segments = pos_segment(sentences)\n",
    "    for i in range(len(pos_segments)):\n",
    "        segments_V.append([])\n",
    "        for k in range(len(pos_segments[i])):\n",
    "            if pos_segments[i][k][1] in ['VERB']:\n",
    "                segments_V[i].append(pos_segments[i][k][0])\n",
    "        \n",
    "    categories_V = []\n",
    "    for i in range(len(segments_V)):\n",
    "        for k in range(len(segments_V[i])):\n",
    "            temp = wordnet.synsets(segments_V[i][k], pos=wordnet.VERB)\n",
    "            if temp:\n",
    "                categories_V.append(temp[0].lemmas()[0].key().split(':', 2)[1])\n",
    "                \n",
    "    bi_loc = [([idx for idx, val in enumerate(verb_topic_list) if val == sub] if sub in verb_topic_list else [None]) for sub in categories_V]\n",
    "\n",
    "    topic_embedding_verb = np.zeros(len(verb_topic_list))\n",
    "    num_seg = len(bi_loc) \n",
    "    for j in range(num_seg):\n",
    "        if num_seg == 0:\n",
    "            topic_embedding_verb=np.zeros(len(verb_topic_list))\n",
    "        else:\n",
    "            index = bi_loc[j]\n",
    "            topic_embedding_verb[index] += 1/num_seg\n",
    "\n",
    "    return(topic_embedding_verb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute custom noun embeddings\n",
    "def custom_noun_embeddings(sentences):\n",
    "    noun_topic_list = [\"%.2d\" % i for i in range(3,29)]\n",
    "    segments_N = []\n",
    "    pos_segments = pos_segment(sentences)\n",
    "    for i in range(len(pos_segments)):\n",
    "        segments_N.append([])\n",
    "        for k in range(len(pos_segments[i])):\n",
    "            if pos_segments[i][k][1] in ['NOUN']:\n",
    "                segments_N[i].append(pos_segments[i][k][0])\n",
    "        \n",
    "    categories_N = []\n",
    "    for i in range(len(segments_N)):\n",
    "        for k in range(len(segments_N[i])):\n",
    "            temp = wordnet.synsets(segments_N[i][k],pos=wordnet.NOUN)\n",
    "            if temp:\n",
    "                categories_N.append(temp[0].lemmas()[0].key().split(':', 2)[1])\n",
    "  \n",
    "    bi_loc = [([idx for idx, val in enumerate(noun_topic_list) if val == sub] if sub in noun_topic_list else [None]) for sub in categories_N]\n",
    "    topic_embedding_noun = np.zeros(len(noun_topic_list))\n",
    "    num_seg = len(bi_loc) \n",
    "    for j in range(num_seg):\n",
    "        if num_seg == 0:\n",
    "            topic_embedding_noun = np.zeros(len(noun_topic_list))\n",
    "        else:\n",
    "            index = bi_loc[j]\n",
    "            topic_embedding_noun[index] += 1/num_seg\n",
    "    \n",
    "    return(topic_embedding_noun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute attribution via dberta labels\n",
    "def attribution_dberta_labels(trained_dberta_embeddings, sentences,attr_model, trained_data):\n",
    "    data_wo_sw = [remove_stopwords(elem) for elem in sentences]\n",
    "    query_embeddings = attr_model.encode(data_wo_sw)\n",
    "    dist_with_query = []\n",
    "    temp = []\n",
    "    for j in range(len(trained_dberta_embeddings)):\n",
    "        temp.append(util.cos_sim(query_embeddings, trained_dberta_embeddings[j]))\n",
    "    max_dist_new = [np.amax(np.array(elem), axis=1) for elem in temp]\n",
    "    max_dist_index = [np.argmax(np.array(elem), axis=1) for elem in temp]\n",
    "    max_dist_per_document = []\n",
    "    for k in range(len(max_dist_new)):\n",
    "        max_dist_per_document.append([])\n",
    "        for l in range(len(max_dist_new[k])):\n",
    "            max_dist_per_document[k].append((max_dist_new[k][l], (k, max_dist_index[k][l])))\n",
    "    dist_with_query.append([max(max_dist_per_document, key = lambda z: z[i])[i] for i in range(len(max_dist_per_document[0]))])\n",
    "    \n",
    "    min_dist = dist_with_query[0]\n",
    "    average_per_document = np.mean([(a_tuple[0]) for a_tuple in min_dist])\n",
    "    min_distance_documents = [(a_tuple[1]) for a_tuple in min_dist]\n",
    "    min_distance_labels = [trained_data['News'][elem[0]] for elem in min_distance_documents]\n",
    "    \n",
    "    Attribution_labels = []\n",
    "    Attribution_labels_Normalised = []\n",
    "    beta = 0.91\n",
    "    for i in range(len(min_dist)):\n",
    "        if min_dist[i][0] >= beta * average_per_document:\n",
    "            if min_distance_labels[i] == 1:\n",
    "                Attribution_labels.append((min_dist[i][0], \"Real\", min_dist[i][1]))\n",
    "                Attribution_labels_Normalised.append((min_dist[i][0], \"Real\", min_dist[i][1]))\n",
    "            else:\n",
    "                Attribution_labels.append((min_dist[i][0], \"Fake\", min_dist[i][1]))\n",
    "                Attribution_labels_Normalised.append((min_dist[i][0], \"Fake\", min_dist[i][1]))\n",
    "        else:\n",
    "            Attribution_labels.append((0, \"Unknown\", \"No Similar Source\"))\n",
    "            \n",
    "    return [dist_with_query, Attribution_labels, Attribution_labels_Normalised]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute attribution via dberta embeddings\n",
    "def attribution_dberta_embeddings(Attribution_labels, Attribution_labels_Normalised):\n",
    "    Count_labels = ['Fake', 'Real', 'Unknown']\n",
    "    bi_loc = [([idx for idx, val in enumerate(Count_labels) if val == sub[1]] if sub[1] in Count_labels else [None]) for sub in Attribution_labels]\n",
    "    Unigram_Sentence_embedding = np.zeros(len(Count_labels))\n",
    "    num_seg = len(bi_loc) \n",
    "    for i in range(num_seg):\n",
    "        if num_seg == 0:\n",
    "            Unigram_Sentence_embedding = np.zeros(len(Count_labels))\n",
    "        else:\n",
    "            index = bi_loc[i]\n",
    "            Unigram_Sentence_embedding[index] += 1/num_seg\n",
    "             \n",
    "    out = defaultdict(list)\n",
    "    for i in Attribution_labels_Normalised:\n",
    "        out[i[1]] += [i[0]]\n",
    "        \n",
    "    labels = ['Fake', 'Real']\n",
    "    Attribution_Normalised = []\n",
    "    total = sum([sum(elem) for elem in out.values()])\n",
    "    if total == 0:\n",
    "        Attribution_Normalised.append([(0, k) for k, v in out.items()])\n",
    "    else:\n",
    "        Attribution_Normalised.append([(sum(v)/total, k) for k, v in out.items()])\n",
    "\n",
    "    bi_loc =[([idx for idx, val in enumerate(labels) if val == sub[1]] if sub[1] in labels else [None]) for sub in Attribution_Normalised[0]]\n",
    "    \n",
    "    Unigram_Sentence_embedding_weighted = np.zeros(len(labels))\n",
    "    num_seg = len(bi_loc) \n",
    "    for i in range(num_seg):\n",
    "        if num_seg == 0:\n",
    "            Unigram_Sentence_embedding_weighted = np.zeros(len(labels))\n",
    "        else:\n",
    "            index = bi_loc[i]\n",
    "            Unigram_Sentence_embedding_weighted[index] += Attribution_Normalised[0][i][0]\n",
    "\n",
    "    return(Unigram_Sentence_embedding, Unigram_Sentence_embedding_weighted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute attribution via doc2vec labels\n",
    "def attribution_doc2vec_labels(sentences, mapping_sent_doc, doc2vec_model, trained_data):\n",
    "    data_wo_sw = [remove_stopwords(elem) for elem in sentences]\n",
    "    tokenized_sent = [word_tokenize(elem.lower()) for elem in data_wo_sw]\n",
    "    for i in range(len(tokenized_sent)):\n",
    "        tokenized_sent[i] = [i for i in tokenized_sent[i] if i.isalnum()]\n",
    "\n",
    "    doc2vec_embeddings = [doc2vec_model.infer_vector(elem) for elem in tokenized_sent]\n",
    "    most_similar_doc2vec = [doc2vec_model.docvecs.most_similar(positive = [elem])[1] for elem in doc2vec_embeddings]\n",
    "    min_dist = [(elem[1],mapping_sent_doc.iloc[elem[0]][1]) for elem in most_similar_doc2vec]\n",
    "    average_per_document = np.mean([(a_tuple[0]) for a_tuple in min_dist])\n",
    "    min_distance_documents = [(a_tuple[1]) for a_tuple in min_dist]\n",
    "    min_distance_labels = [trained_data['News'][elem[0]] for elem in min_distance_documents]\n",
    "    \n",
    "    Attribution_labels = []\n",
    "    Attribution_labels_Normalised = []\n",
    "    beta = 0.96\n",
    "    for i in range(len(min_dist)):\n",
    "        if min_dist[i][0] >= beta * average_per_document:\n",
    "            if min_distance_labels[i] == 1:\n",
    "                Attribution_labels.append((min_dist[i][0], \"Real\", min_dist[i][1]))\n",
    "                Attribution_labels_Normalised.append((min_dist[i][0], \"Real\", min_dist[i][1]))\n",
    "            else:\n",
    "                Attribution_labels.append((min_dist[i][0], \"Fake\", min_dist[i][1]))\n",
    "                Attribution_labels_Normalised.append((min_dist[i][0], \"Fake\", min_dist[i][1]))\n",
    "        else:\n",
    "            Attribution_labels.append((0, \"Unknown\", \"No Similar Source\"))\n",
    "    \n",
    "    return(Attribution_labels, Attribution_labels_Normalised)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute attribution via doc2vec embeddings\n",
    "def attribution_doc2vec_embeddings(Attribution_labels, Attribution_labels_Normalised):\n",
    "    Count_labels = ['Fake', 'Real', 'Unknown']\n",
    "    bi_loc = [([idx for idx, val in enumerate(Count_labels) if val == sub[1]] if sub[1] in Count_labels else [None]) for sub in Attribution_labels]\n",
    "    Unigram_Sentence_embedding = np.zeros(len(Count_labels))\n",
    "    num_seg = len(bi_loc) \n",
    "    for i in range(num_seg):\n",
    "        if num_seg == 0:\n",
    "            Unigram_Sentence_embedding = np.zeros(len(Count_labels))\n",
    "        else:\n",
    "            index = bi_loc[i]\n",
    "            Unigram_Sentence_embedding[index] += 1/num_seg\n",
    "            \n",
    "    out = defaultdict(list)\n",
    "    for i in Attribution_labels_Normalised:\n",
    "        out[i[1]] += [i[0]]\n",
    "        \n",
    "    labels = ['Fake', 'Real']\n",
    "    Attribution_Normalised = []\n",
    "    total = sum([sum(elem) for elem in out.values()])\n",
    "    if total == 0:\n",
    "        Attribution_Normalised.append([(0, k) for k, v in out.items()])\n",
    "    else:\n",
    "        Attribution_Normalised.append([(sum(v)/total, k) for k, v in out.items()])\n",
    "\n",
    "    bi_loc = [([idx for idx, val in enumerate(labels) if val == sub[1]] if sub[1] in labels else [None]) for sub in Attribution_Normalised[0]]\n",
    "    \n",
    "    Unigram_Sentence_embedding_weighted = np.zeros(len(labels))\n",
    "    num_seg = len(bi_loc) \n",
    "    for i in range(num_seg):\n",
    "        if num_seg == 0:\n",
    "            Unigram_Sentence_embedding_weighted = np.zeros(len(labels))\n",
    "        else:\n",
    "            index = bi_loc[i]\n",
    "            Unigram_Sentence_embedding_weighted[index] += Attribution_Normalised[0][i][0]\n",
    "\n",
    "    return(Unigram_Sentence_embedding, Unigram_Sentence_embedding_weighted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Markov structure\n",
    "def markov_structure(sentences, markov_real, markov_fake):\n",
    "    POS_tags_sentences = []\n",
    "    for elem in sentences:\n",
    "        temp = TextBlob(elem)\n",
    "        POS_tags_sentences.append([elem[1] for elem in temp.tags])\n",
    "    for j in range(len(POS_tags_sentences)):\n",
    "        POS_tags_sentences[j].insert(0, 'Start')\n",
    "        POS_tags_sentences[j].append('End')\n",
    "        \n",
    "    bi_segments = []\n",
    "    for elem in POS_tags_sentences:\n",
    "        bi_segments.append(list(zip(elem, elem[1:])))\n",
    "        \n",
    "    test_real = []\n",
    "    for i in range(len(bi_segments)):\n",
    "        trans_prob = []\n",
    "        for j in range(len(bi_segments[i])):\n",
    "            temp = markov_real[markov_real['Bigram'] == bi_segments[i][j]]['norm_prob']\n",
    "            if len(temp.values) != 0:\n",
    "                trans_prob.append(temp.values[0])\n",
    "        final_trans_prob = np.prod(trans_prob)\n",
    "        test_real.append(final_trans_prob)\n",
    "        \n",
    "    test_fake = []\n",
    "    for i in range(len(bi_segments)):\n",
    "        trans_prob = []\n",
    "        for j in range(len(bi_segments[i])):\n",
    "            temp = markov_fake[markov_fake['Bigram'] == bi_segments[i][j]]['norm_prob']\n",
    "            if len(temp.values) != 0:\n",
    "                trans_prob.append(temp.values[0])\n",
    "        final_trans_prob = np.prod(trans_prob)\n",
    "        test_fake.append(final_trans_prob)  \n",
    "        \n",
    "    df_markov = pd.DataFrame({'real': test_real, 'fake': test_fake})\n",
    "    df_markov['Pred'] = np.where(df_markov['real'] > df_markov['fake'], 1,0)\n",
    "    \n",
    "    markov_prediction = df_markov['Pred'].mode()\n",
    "    if len(markov_prediction) != 1:\n",
    "        markov_prediction = 0 \n",
    "    else:\n",
    "        markov_prediction = markov_prediction[0]\n",
    "\n",
    "    return(markov_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute x_attribute feature\n",
    "def x_attribute_feature(attr_labels, topic_labels, Topic_Attribution_Labels, Topic_Attribution_Labels_type1, topics_for_sentences, zero_topics_for_sentences, zero_topic_labels, Zero_Topic_Attribution_Labels, Zero_Topic_Attribution_Labels_type1):\n",
    "    \n",
    "    attribute_labels = attr_labels\n",
    "    attribute_labels = [elem[1] for elem in attribute_labels]\n",
    "            \n",
    "    Topic_Labels = topic_labels     \n",
    "    Topic_Attr_labels = Topic_Attribution_Labels\n",
    "    Topic_Attribution_Labels_type1 = Topic_Attribution_Labels_type1\n",
    "    \n",
    "    uni_topic_sentences = topics_for_sentences\n",
    "    uni_topic_sentences = [Topic_Labels[elem] for elem in uni_topic_sentences]\n",
    "    \n",
    "    Topics_Bigram = list(zip(uni_topic_sentences, uni_topic_sentences[1:]))\n",
    "    Topics_Bigram.append((uni_topic_sentences[-1], 'End'))\n",
    "    \n",
    "    Topics_Attribution = [item1 + (item2,) for (item1, item2) in zip(Topics_Bigram, attribute_labels)]\n",
    "    Topics_Attribution_type1 = [(elem[1], elem[2]) for elem in Topics_Attribution]\n",
    "    Topics_Attribution = [elem for elem in Topics_Attribution if elem[2] != 'Unknown']\n",
    "    Topics_Attribution_type1 = [elem for elem in Topics_Attribution_type1 if elem[1] != 'Unknown']\n",
    "    \n",
    "    bi_loc = [([idx for idx,val in enumerate(Topic_Attr_labels) if val == sub] if sub in Topic_Attr_labels else [None]) for sub in Topics_Attribution]\n",
    "    Topics_Attribution_embedding = np.zeros(len(Topic_Attr_labels))\n",
    "    num_seg = len(bi_loc) \n",
    "    for j in range(num_seg):\n",
    "        if num_seg == 0:\n",
    "            Topics_Attribution_embedding = np.zeros(len(Topic_Attr_labels))\n",
    "        else:\n",
    "            index = bi_loc[j]\n",
    "            Topics_Attribution_embedding[index] += 1/num_seg\n",
    "            \n",
    "    bi_loc = [([idx for idx,val in enumerate(Topic_Attribution_Labels_type1) if val == sub] if sub in Topic_Attribution_Labels_type1 else [None]) for sub in Topics_Attribution_type1]\n",
    "    Topics_Attribution_embedding_type1 = np.zeros(len(Topic_Attribution_Labels_type1))\n",
    "    num_seg = len(bi_loc) \n",
    "    for j in range(num_seg):\n",
    "        if num_seg == 0:\n",
    "            Topics_Attribution_embedding_type1 = np.zeros(len(Topic_Attribution_Labels_type1))\n",
    "        else:\n",
    "            index = bi_loc[j]\n",
    "            Topics_Attribution_embedding_type1[index] += 1/num_seg \n",
    "            \n",
    "    Zero_Topic_Labels = zero_topic_labels     \n",
    "    Zero_Topic_Attr_labels = Zero_Topic_Attribution_Labels\n",
    "    Zero_Topic_Attribution_Labels_type1 = Zero_Topic_Attribution_Labels_type1\n",
    "    \n",
    "    Zero_uni_topic_sentences = zero_topics_for_sentences\n",
    "    \n",
    "    Zero_Topics_Bigram = list(zip(Zero_uni_topic_sentences, Zero_uni_topic_sentences[1:]))\n",
    "    Zero_Topics_Bigram.append((Zero_uni_topic_sentences[-1], 'End'))\n",
    "    \n",
    "    Zero_Topics_Attribution = [item1 + (item2,) for (item1, item2) in zip(Zero_Topics_Bigram, attribute_labels)]\n",
    "    Zero_Topics_Attribution_type1 = [(elem[1], elem[2]) for elem in Zero_Topics_Attribution]\n",
    "    \n",
    "    Zero_Topics_Attribution = [elem for elem in Zero_Topics_Attribution if elem[2] != 'Unknown']\n",
    "    Zero_Topics_Attribution_type1 = [elem for elem in Zero_Topics_Attribution_type1 if elem[1] != 'Unknown']\n",
    "    \n",
    "    bi_loc = [([idx for idx,val in enumerate(Zero_Topic_Attr_labels) if val == sub] if sub in Zero_Topic_Attr_labels else [None]) for sub in Zero_Topics_Attribution]\n",
    "    Zero_Topics_Attribution_embedding = np.zeros(len(Zero_Topic_Attr_labels))\n",
    "    num_seg = len(bi_loc) \n",
    "    for j in range(num_seg):\n",
    "        if num_seg == 0:\n",
    "            Zero_Topics_Attribution_embedding = np.zeros(len(Zero_Topic_Attr_labels))\n",
    "        else:\n",
    "            index = bi_loc[j]\n",
    "            Zero_Topics_Attribution_embedding[index] += 1/num_seg\n",
    "            \n",
    "    bi_loc = [([idx for idx, val in enumerate(Zero_Topic_Attribution_Labels_type1) if val == sub] if sub in Zero_Topic_Attribution_Labels_type1 else [None]) for sub in Zero_Topics_Attribution_type1]\n",
    "    Zero_Topics_Attribution_embedding_type1 = np.zeros(len(Zero_Topic_Attribution_Labels_type1))\n",
    "    num_seg = len(bi_loc) \n",
    "    for j in range(num_seg):\n",
    "        if num_seg == 0:\n",
    "            Zero_Topics_Attribution_embedding_type1 = np.zeros(len(Zero_Topic_Attribution_Labels_type1))\n",
    "        else:\n",
    "            index = bi_loc[j]\n",
    "            Zero_Topics_Attribution_embedding_type1[index] += 1/num_seg  \n",
    "    \n",
    "    return (Topics_Attribution, Topics_Attribution_embedding, Topics_Attribution_type1, Topics_Attribution_embedding_type1, Zero_Topics_Attribution_embedding, Zero_Topics_Attribution_embedding_type1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute x_attribute feature zero-shot learning\n",
    "def x_attribute_feature_wo_zeroshot(attr_labels, topic_labels, Topic_Attribution_Labels, Topic_Attribution_Labels_type1,topics_for_sentences):\n",
    "    attribute_labels = attr_labels\n",
    "    attribute_labels = [elem[1] for elem in attribute_labels]\n",
    "            \n",
    "    Topic_Labels = topic_labels     \n",
    "    Topic_Attr_labels = Topic_Attribution_Labels\n",
    "    Topic_Attribution_Labels_type1 = Topic_Attribution_Labels_type1\n",
    "    \n",
    "    uni_topic_sentences = topics_for_sentences\n",
    "    uni_topic_sentences = [Topic_Labels[elem] for elem in uni_topic_sentences]\n",
    "    \n",
    "    Topics_Bigram =list(zip(uni_topic_sentences, uni_topic_sentences[1:]))\n",
    "    Topics_Bigram.append((uni_topic_sentences[-1], 'End'))\n",
    "    \n",
    "    Topics_Attribution = [item1 + (item2,) for (item1, item2) in zip(Topics_Bigram,attribute_labels)]\n",
    "    Topics_Attribution_type1 = [(elem[1], elem[2]) for elem in Topics_Attribution]\n",
    "    Topics_Attribution = [elem for elem in Topics_Attribution if elem[2] != 'Unknown']\n",
    "    Topics_Attribution_type1 = [elem for elem in Topics_Attribution_type1 if elem[1] != 'Unknown']\n",
    "    \n",
    "    bi_loc = [([idx for idx,val in enumerate(Topic_Attr_labels) if val == sub] if sub in Topic_Attr_labels else [None]) for sub in Topics_Attribution]\n",
    "    Topics_Attribution_embedding = np.zeros(len(Topic_Attr_labels))\n",
    "    num_seg = len(bi_loc) \n",
    "    for j in range(num_seg):\n",
    "        if num_seg == 0:\n",
    "            Topics_Attribution_embedding = np.zeros(len(Topic_Attr_labels))\n",
    "        else:\n",
    "            index = bi_loc[j]\n",
    "            Topics_Attribution_embedding[index] += 1/num_seg\n",
    "            \n",
    "    bi_loc = [([idx for idx,val in enumerate(Topic_Attribution_Labels_type1) if val == sub] if sub in Topic_Attribution_Labels_type1 else [None]) for sub in Topics_Attribution_type1]\n",
    "    Topics_Attribution_embedding_type1 = np.zeros(len(Topic_Attribution_Labels_type1))\n",
    "    num_seg = len(bi_loc) \n",
    "    for j in range(num_seg):\n",
    "        if num_seg == 0:\n",
    "            Topics_Attribution_embedding_type1 = np.zeros(len(Topic_Attribution_Labels_type1))\n",
    "        else:\n",
    "            index = bi_loc[j]\n",
    "            Topics_Attribution_embedding_type1[index] += 1/num_seg \n",
    "\n",
    "    return (Topics_Attribution, Topics_Attribution_embedding, Topics_Attribution_type1, Topics_Attribution_embedding_type1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute x_entailment feature\n",
    "def x_entailment_feature(attr_labels, entailment_feature_label, Attr_Entailment_labels, topic_labels, Topic_Entailment_labels, topics_for_sentences, zero_topics_for_sentences, zero_topic_labels, Zero_Topic_Entailment_labels):\n",
    "    attribute_labels = attr_labels\n",
    "    attribute_labels = [elem[1] for elem in attribute_labels]\n",
    "    attribute_bigram = list(zip(attribute_labels, attribute_labels[1:]))\n",
    "    entailment_labels = entailment_feature_label\n",
    "    \n",
    "    Attribution_Entailment_labels = Attr_Entailment_labels\n",
    "    Attribution_Entailment = [item1 + (item2,) for (item1, item2) in zip(attribute_bigram, entailment_labels)]\n",
    "    Attribution_Entailment = [elem for elem in Attribution_Entailment if (elem[0] != 'Unknown' and elem[1] != 'Unknown')]\n",
    "    bi_loc = [([idx for idx,val in enumerate(Attribution_Entailment_labels) if val == sub] if sub in Attribution_Entailment_labels else [None]) for sub in Attribution_Entailment]\n",
    "    Attr_Entailment_embedding = np.zeros(len(Attribution_Entailment_labels))\n",
    "    num_seg = len(bi_loc) \n",
    "    for j in range(num_seg):\n",
    "        if num_seg == 0:\n",
    "            Attr_Entailment_embedding = np.zeros(len(Attribution_Entailment_labels))\n",
    "        else:\n",
    "            index = bi_loc[j]\n",
    "            Attr_Entailment_embedding[index] += 1/num_seg\n",
    "     \n",
    "    Topic_Labels = topic_labels     \n",
    "    Topic_Entailment_labels = Topic_Entailment_labels\n",
    "    uni_topic_sentences = topics_for_sentences\n",
    "    uni_topic_sentences = [Topic_Labels[elem] for elem in uni_topic_sentences]\n",
    "    Topics_Bigram = list(zip(uni_topic_sentences, uni_topic_sentences[1:]))\n",
    "    Topics_Bigram.append((uni_topic_sentences[-1], 'End'))\n",
    "    Topics_Entailment = [item1 + (item2,) for (item1, item2) in zip(Topics_Bigram, entailment_labels)]\n",
    "    Topics_Entailment = [elem for elem in Topics_Entailment if elem[2]!='NEUTRAL']\n",
    "    \n",
    "    bi_loc = [([idx for idx,val in enumerate(Topic_Entailment_labels) if val == sub] if sub in Topic_Entailment_labels else [None]) for sub in Topics_Entailment]\n",
    "    Topics_Entailment_embedding = np.zeros(len(Topic_Entailment_labels))\n",
    "    num_seg = len(bi_loc) \n",
    "    for j in range(num_seg):\n",
    "        if num_seg == 0:\n",
    "            Topics_Entailment_embedding = np.zeros(len(Topic_Entailment_labels))\n",
    "        else:\n",
    "            index = bi_loc[j]\n",
    "            Topics_Entailment_embedding[index] += 1/num_seg\n",
    "    \n",
    "    Zero_Topic_Labels = zero_topic_labels     \n",
    "    Zero_Topic_Entailment_labels = Zero_Topic_Entailment_labels\n",
    "    Zero_uni_topic_sentences = zero_topics_for_sentences\n",
    "\n",
    "    Zero_Topics_Bigram = list(zip(Zero_uni_topic_sentences, Zero_uni_topic_sentences[1:]))\n",
    "    Zero_Topics_Bigram.append((Zero_uni_topic_sentences[-1], 'End'))\n",
    "    Zero_Topics_Entailment = [item1 + (item2,) for (item1, item2) in zip(Zero_Topics_Bigram, entailment_labels)]\n",
    "    Zero_Topics_Entailment = [elem for elem in Zero_Topics_Entailment if elem[2] != 'NEUTRAL']\n",
    "    \n",
    "    bi_loc = [([idx for idx,val in enumerate(Zero_Topic_Entailment_labels) if val == sub] if sub in Zero_Topic_Entailment_labels else [None]) for sub in Zero_Topics_Entailment]\n",
    "    Zero_Topics_Entailment_embedding = np.zeros(len(Zero_Topic_Entailment_labels))\n",
    "    num_seg = len(bi_loc) \n",
    "    for j in range(num_seg):\n",
    "        if num_seg == 0:\n",
    "            Zero_Topics_Entailment_embedding = np.zeros(len(Zero_Topic_Entailment_labels))\n",
    "        else:\n",
    "            index = bi_loc[j]\n",
    "            Zero_Topics_Entailment_embedding[index] += 1/num_seg\n",
    "            \n",
    "    return(Attribution_Entailment, Attr_Entailment_embedding, Topics_Entailment, Topics_Entailment_embedding, Zero_Topics_Entailment_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute x_entailment feature zero-shot learning\n",
    "def x_entailment_feature_wo_zeroshot(attr_labels, entailment_feature_label, Attr_Entailment_labels, topic_labels, Topic_Entailment_labels, topics_for_sentences):\n",
    "    attribute_labels = attr_labels\n",
    "    attribute_labels = [elem[1] for elem in attribute_labels]\n",
    "    attribute_bigram = list(zip(attribute_labels, attribute_labels[1:]))\n",
    "    entailment_labels = entailment_feature_label\n",
    "    \n",
    "    Attribution_Entailment_labels = Attr_Entailment_labels\n",
    "    Attribution_Entailment = [item1 + (item2,) for (item1, item2) in zip(attribute_bigram, entailment_labels)]\n",
    "    Attribution_Entailment = [elem for elem in Attribution_Entailment if (elem[0] != 'Unknown' and elem[1] != 'Unknown')]\n",
    "    bi_loc = [([idx for idx,val in enumerate(Attribution_Entailment_labels) if val == sub] if sub in Attribution_Entailment_labels else [None]) for sub in Attribution_Entailment]\n",
    "    Attr_Entailment_embedding = np.zeros(len(Attribution_Entailment_labels))\n",
    "    num_seg = len(bi_loc) \n",
    "    for j in range(num_seg):\n",
    "        if num_seg == 0:\n",
    "            Attr_Entailment_embedding = np.zeros(len(Attribution_Entailment_labels))\n",
    "        else:\n",
    "            index = bi_loc[j]\n",
    "            Attr_Entailment_embedding[index] += 1/num_seg\n",
    "    \n",
    "    Topic_Labels = topic_labels     \n",
    "    Topic_Entailment_labels = Topic_Entailment_labels\n",
    "    uni_topic_sentences = topics_for_sentences\n",
    "    uni_topic_sentences = [Topic_Labels[elem] for elem in uni_topic_sentences]\n",
    "    Topics_Bigram =list(zip(uni_topic_sentences, uni_topic_sentences[1:]))\n",
    "    Topics_Bigram.append((uni_topic_sentences[-1], 'End'))\n",
    "    Topics_Entailment = [item1 + (item2,) for (item1, item2) in zip(Topics_Bigram, entailment_labels)]\n",
    "    Topics_Entailment = [elem for elem in Topics_Entailment if elem[2] != 'NEUTRAL']\n",
    "    \n",
    "    bi_loc = [([idx for idx,val in enumerate(Topic_Entailment_labels) if val == sub] if sub in Topic_Entailment_labels else [None]) for sub in Topics_Entailment]\n",
    "    Topics_Entailment_embedding = np.zeros(len(Topic_Entailment_labels))\n",
    "    num_seg = len(bi_loc) \n",
    "    for j in range(num_seg):\n",
    "        if num_seg == 0:\n",
    "            Topics_Entailment_embedding = np.zeros(len(Topic_Entailment_labels))\n",
    "        else:\n",
    "            index = bi_loc[j]\n",
    "            Topics_Entailment_embedding[index] += 1/num_seg\n",
    "            \n",
    "    return(Attribution_Entailment, Attr_Entailment_embedding, Topics_Entailment, Topics_Entailment_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute hypernym and hyponym\n",
    "def Hypernym_Hyponym(chunk_sentences):\n",
    "    Word_Category = []\n",
    "    for i in range(len(chunk_sentences)):\n",
    "        Word_Category.append([])\n",
    "        word_level = word_tokenize(chunk_sentences[i])\n",
    "        for k in range(len(word_level)):\n",
    "            temp = lesk(word_level,word_level[k])\n",
    "            if temp:\n",
    "                temp_hypernym = len(temp.hypernym_paths()[0])\n",
    "                temp_hyponym = len(temp.hyponyms())\n",
    "                if temp_hyponym == 0 :\n",
    "                    if temp_hypernym == 1:\n",
    "                         Word_Category[i].append(\"General\")\n",
    "                    else:\n",
    "                          Word_Category[i].append(\"Specific\")\n",
    "                else:\n",
    "                    if temp_hypernym == 1:\n",
    "                         Word_Category[i].append(\"General\")\n",
    "                    else:\n",
    "                        Word_Category[i].append(\"Between\")\n",
    "                        \n",
    "    Word_Category_Counter = []\n",
    "    for j in range(len(Word_Category)):\n",
    "        counter = Counter(Word_Category[j])\n",
    "        total = sum(counter.values())\n",
    "        for item, count in counter.items():\n",
    "            counter[item] /= total\n",
    "        Word_Category_Counter.append(sorted(counter.items()))\n",
    "        \n",
    "    wordnet_labels = ['Between', 'General', 'Specific']\n",
    "    \n",
    "    for i in range(len(Word_Category_Counter)):\n",
    "        elem=[k[0] for k in Word_Category_Counter[i]]\n",
    "        if wordnet_labels not in elem:\n",
    "            non_intersection = set(wordnet_labels) - set(elem)\n",
    "            for item in non_intersection:\n",
    "                Word_Category_Counter[i].append((item,0))\n",
    "                \n",
    "    if (len(Word_Category_Counter)) == 1 :\n",
    "        Word_Category_Counter.append([('Between', 0), ('General', 0), ('Specific', 0)])\n",
    "        Word_Category_Counter.append([('Between', 0), ('General', 0), ('Specific', 0)])\n",
    "    if (len(Word_Category_Counter)) == 2 :\n",
    "        Word_Category_Counter.append([('Between', 0), ('General', 0), ('Specific', 0)])\n",
    "    \n",
    "    df_Hypernym_Hyponym = pd.DataFrame([x[1] for x in Word_Category_Counter[0]]+[x[1] for x in Word_Category_Counter[1]]+[x[1] for x in Word_Category_Counter[2]]).T\n",
    "    \n",
    "    df_Hypernym_Hyponym.columns = ['B1', 'G1', 'S1', 'B2', 'G2', 'S2', 'B3', 'G3', 'S3']\n",
    "\n",
    "    return(Word_Category, df_Hypernym_Hyponym)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entailment thread\n",
    "def entailment_thread(req_id, sentences,sentence_pairs):\n",
    "    global file_locker\n",
    "\n",
    "    entailment_feature_label, df_entailment_type1, df_entailment_type2 = entailment_features(sentence_pairs, sentences)\n",
    "    df_entailment_type2.columns = [elem + \"_norm\" for elem in df_entailment_type2.columns]\n",
    "    \n",
    "    # Send message\n",
    "    file_locker.acquire_file_lock('tmp/' + str(req_id) + \".pickle\")\n",
    "    stored_requests[req_id][\"message\"] = \"Wohoo! Entailment is done\"\n",
    "    with open('tmp/' + str(req_id) + \".pickle\", \"wb\") as handle:\n",
    "        pickle.dump(stored_requests[req_id], handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    file_locker.delete_file_lock('tmp/' + str(req_id) + \".pickle\")\n",
    "    \n",
    "    return (entailment_feature_label, df_entailment_type1, df_entailment_type2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Personality thread\n",
    "def personality_thread(req_id, data, vec_for_personality, model_for_personality, sentences):\n",
    "    global file_locker\n",
    "\n",
    "    df_personality = personality_feature(data, vec_for_personality, model_for_personality)\n",
    "    df_sentiment_type1,df_sentiment_type2 = sentiment_feature(sentences) \n",
    "    df_sentiment_type2.columns = [elem + \"_norm\" for elem in df_sentiment_type2.columns]\n",
    "    \n",
    "    # Send message\n",
    "    file_locker.acquire_file_lock('tmp/' + str(req_id) + \".pickle\")\n",
    "    stored_requests[req_id][\"message\"] = \"Wohoo! We figured out the sentiments and personality\"\n",
    "    with open('tmp/' + str(req_id) + \".pickle\", \"wb\") as handle:\n",
    "        pickle.dump(stored_requests[req_id], handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    file_locker.delete_file_lock('tmp/' + str(req_id) + \".pickle\")\n",
    "    \n",
    "    return (df_personality, df_sentiment_type1, df_sentiment_type2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Topic Features thread\n",
    "def topic_features_thread(req_id, data, sentences, dberta_model, model_for_topic):\n",
    "    global file_locker\n",
    "\n",
    "    raw_doctopic_embedding, normalised_doctopic_embedding, topics_for_sentences = topic_feature(data, sentences, dberta_model, model_for_topic)\n",
    "    df_raw_doctopic = pd.DataFrame(raw_doctopic_embedding).T\n",
    "    df_normalised_doctopic = pd.DataFrame(normalised_doctopic_embedding).T\n",
    "    df_raw_doctopic.columns = ['CRIME', 'DISSENT', 'DOMESTIC ECONOMIC', 'DOMESTIC POLITICAL', 'ENERGY', 'ENVIRONMENT', 'HEALTH', 'HUMAN RIGHTS', 'INTERNATIONAL ECONOMIC', 'INTERNATIONAL POLITICAL', 'LEADER', 'MEDIA', 'MIGRATION', 'MILITARY', 'NARCOTICS', 'OTHER', 'PEACEKEEPING', 'PROLIFERATION', 'TECHNOLOGY', 'TELECOM', 'TERRORISM', 'TERRORISM.911USA', 'TERRORISM.911WEB', 'URGENT']\n",
    "\n",
    "    df_normalised_doctopic.columns = ['CRIME_Norm', 'DISSENT_Norm', 'DOMESTIC ECONOMIC_Norm', 'DOMESTIC POLITICAL_Norm', 'ENERGY_Norm', 'ENVIRONMENT_Norm', 'HEALTH_Norm', 'HUMAN RIGHTS_Norm', 'INTERNATIONAL ECONOMIC_Norm', 'INTERNATIONAL POLITICAL_Norm', 'LEADER_Norm', 'MEDIA_Norm', 'MIGRATION_Norm', 'MILITARY_Norm', 'NARCOTICS_Norm', 'OTHER_Norm', 'PEACEKEEPING_Norm', 'PROLIFERATION_Norm', 'TECHNOLOGY_Norm', 'TELECOM_Norm', 'TERRORISM_Norm', 'TERRORISM.911USA_Norm', 'TERRORISM.911WEB_Norm', 'URGENT_Norm']\n",
    "    \n",
    "    # Send message\n",
    "    file_locker.acquire_file_lock('tmp/' + str(req_id) + \".pickle\")\n",
    "    stored_requests[req_id][\"message\"] = \"Pheww!! Getting out the topics was time consuming\"\n",
    "    with open('tmp/' + str(req_id) + \".pickle\", \"wb\") as handle:\n",
    "        pickle.dump(stored_requests[req_id], handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    file_locker.delete_file_lock('tmp/' + str(req_id) + \".pickle\")\n",
    "    \n",
    "    return (df_raw_doctopic, df_normalised_doctopic, topics_for_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero-shot learning topics thread\n",
    "def zeroshot_topic_thread(req_id, data):\n",
    "    global file_locker\n",
    "\n",
    "    df_doc_zero_topics,zero_topics_for_sentences = zero_shot_learning_topic(data)\n",
    "    # Send message\n",
    "    file_locker.acquire_file_lock('tmp/' + str(req_id) + \".pickle\")\n",
    "    stored_requests[req_id][\"message\"] = \"Oh my zero shot learning yeilded results at 0 km/h\"\n",
    "    with open('tmp/' + str(req_id) + \".pickle\", \"wb\") as handle:\n",
    "        pickle.dump(stored_requests[req_id], handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    file_locker.delete_file_lock('tmp/' + str(req_id) + \".pickle\")\n",
    "    \n",
    "    return (df_doc_zero_topics, zero_topics_for_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Embedding thread\n",
    "def custom_emb_thread(req_id, sentences, models_for_adj, glove_embeddings):\n",
    "    global file_locker\n",
    "\n",
    "    adj_embeddings = custom_adjective_embeddings(sentences, models_for_adj, glove_embeddings)\n",
    "    df_adj_embeddings = pd.DataFrame(adj_embeddings).T\n",
    "    df_adj_embeddings.columns = [\"adj_category\" + str(i+1) for i in range(len(adj_embeddings))]\n",
    "    \n",
    "    verb_embeddings = custom_verb_embeddings(sentences)\n",
    "    df_verb_embeddings = pd.DataFrame(verb_embeddings).T\n",
    "    df_verb_embeddings.columns = [\"verb_category\" + str(i+1) for i in range(len(verb_embeddings))]\n",
    "    \n",
    "    noun_embeddings = custom_noun_embeddings(sentences)\n",
    "    df_noun_embeddings = pd.DataFrame(noun_embeddings).T\n",
    "    df_noun_embeddings.columns = [\"noun_category\" + str(i+1) for i in range(len(noun_embeddings))]\n",
    "    \n",
    "    # Send message\n",
    "    file_locker.acquire_file_lock('tmp/' + str(req_id) + \".pickle\")\n",
    "    stored_requests[req_id][\"message\"] = \"Yaya! some crafty custom embeddings are here!\"\n",
    "    with open('tmp/' + str(req_id) + \".pickle\", \"wb\") as handle:\n",
    "        pickle.dump(stored_requests[req_id], handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    file_locker.delete_file_lock('tmp/' + str(req_id) + \".pickle\")\n",
    "    \n",
    "    return (df_adj_embeddings, df_verb_embeddings, df_noun_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Markov thread\n",
    "def markov_thread(req_id, sentences, trained_markov_real_structure, trained_markov_fake_structure):\n",
    "    global file_locker\n",
    "\n",
    "    df_markov_prediction = pd.DataFrame([markov_structure(sentences, trained_markov_real_structure, trained_markov_fake_structure)],columns=['markov_pred'])\n",
    "    \n",
    "    # Send message\n",
    "    file_locker.acquire_file_lock('tmp/' + str(req_id) + \".pickle\")\n",
    "    stored_requests[req_id][\"message\"] = \"The Markov Mystery is here as well!\"\n",
    "    with open('tmp/' + str(req_id) + \".pickle\", \"wb\") as handle:\n",
    "        pickle.dump(stored_requests[req_id], handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    file_locker.delete_file_lock('tmp/' + str(req_id) + \".pickle\")\n",
    "    \n",
    "    return df_markov_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attribution thread\n",
    "def attribution_thread(req_id, trained_dberta_embeddings, sentences, attr_model, trained_data, doc2vec_model, mapping_sent_doc):\n",
    "    global file_locker\n",
    "\n",
    "    attr_dist_from_trained_data, attr_labels, attr_labels_weighted = attribution_dberta_labels(trained_dberta_embeddings, sentences, attr_model, trained_data)\n",
    "    attr_embedding, attr_embedding_weighted = attribution_dberta_embeddings(attr_labels, attr_labels_weighted)\n",
    "    \n",
    "    df_attr_embedding = pd.DataFrame(attr_embedding).T\n",
    "    df_attr_embedding_weighted = pd.DataFrame(attr_embedding_weighted).T\n",
    "    \n",
    "    df_attr_embedding.columns = ['Fake', 'Real', 'Unknown_1']\n",
    "    df_attr_embedding_weighted.columns = ['Fake_weight', 'Real_weight']\n",
    "    \n",
    "    attr_labels_doc2vec, attr_labels_weighted_doc2vec = attribution_doc2vec_labels(sentences, mapping_sent_doc, doc2vec_model, trained_data)\n",
    "    attr_embedding_doc2vec, attr_embedding_weighted_doc2vec = attribution_doc2vec_embeddings(attr_labels_doc2vec, attr_labels_weighted_doc2vec)\n",
    "    \n",
    "    df_attr_embedding_doc2vec = pd.DataFrame(attr_embedding_doc2vec).T\n",
    "    df_attr_embedding_weighted_doc2vec = pd.DataFrame(attr_embedding_weighted_doc2vec).T\n",
    "    \n",
    "    df_attr_embedding_doc2vec.columns = ['Fake_doc2vec', 'Real_doc2vec', 'Unknown_doc2vec']\n",
    "    df_attr_embedding_weighted_doc2vec.columns = ['Fake_doc2vec_wt', 'Real_doc2vec_wt']\n",
    "    \n",
    "    # Send message\n",
    "    file_locker.acquire_file_lock('tmp/' + str(req_id) + \".pickle\")\n",
    "    stored_requests[req_id][\"message\"] = \"Well your unmasking is attributed to attribution\"\n",
    "    with open('tmp/' + str(req_id) + \".pickle\", \"wb\") as handle:\n",
    "        pickle.dump(stored_requests[req_id], handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    file_locker.delete_file_lock('tmp/' + str(req_id) + \".pickle\")\n",
    "    \n",
    "    return (attr_labels, df_attr_embedding, df_attr_embedding_weighted, df_attr_embedding_doc2vec, df_attr_embedding_weighted_doc2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# High-level features extraction\n",
    "def high_level_features(req_id, text, vec_for_personality, model_for_personality, dberta_model, model_for_topic, models_for_adj, glove_embeddings, trained_markov_real_structure, trained_markov_fake_structure, trained_dberta_embeddings, trained_data, mapping_sent_doc, doc2vec_model, seq_of_col, Entailment_Classifier, Sentiment_Classifier, attr_model, zero_classifier):\n",
    "    global file_locker\n",
    "\n",
    "    # Send message\n",
    "    file_locker.acquire_file_lock('tmp/' + str(req_id) + \".pickle\")\n",
    "    stored_requests[req_id][\"message\"] = \"Keep Calm and let the magic of 2nd pipeline take over\"\n",
    "    with open('tmp/' + str(req_id) + \".pickle\", \"wb\") as handle:\n",
    "        pickle.dump(stored_requests[req_id], handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    file_locker.delete_file_lock('tmp/' + str(req_id) + \".pickle\")\n",
    "    \n",
    "    # Data Preparation\n",
    "    data = text\n",
    "    sentences,sentence_pairs = prepare_sentences_pipeline2(data)\n",
    "    chunks = chunk_sentences(sentences)\n",
    "     \n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=8) as executor:\n",
    "        ent_thread_future = executor.submit(entailment_thread, req_id, sentence_pairs, sentences)\n",
    "        per_thread_future = executor.submit(personality_thread, req_id, data, vec_for_personality, model_for_personality, sentences)\n",
    "        top_thread_future = executor.submit(topic_features_thread, req_id, data, sentences, dberta_model, model_for_topic)\n",
    "        zer_thread_future = executor.submit(zeroshot_topic_thread, req_id, data)\n",
    "        emb_thread_future = executor.submit(custom_emb_thread, req_id, sentences, models_for_adj, glove_embeddings)\n",
    "        mar_thread_future = executor.submit(markov_thread, req_id, sentences, trained_markov_real_structure, trained_markov_fake_structure)\n",
    "        att_thread_future = executor.submit(attribution_thread, req_id, trained_dberta_embeddings, sentences, attr_model, trained_data, doc2vec_model, mapping_sent_doc)\n",
    "        \n",
    "    # Entailments\n",
    "    entailment_feature_label, df_entailment_type1, df_entailment_type2 = ent_thread_future.result()\n",
    "    \n",
    "    # Personality and Sentiment\n",
    "    df_personality, df_sentiment_type1, df_sentiment_type2 = per_thread_future.result()\n",
    "    \n",
    "    # Topics\n",
    "    df_raw_doctopic, df_normalised_doctopic, topics_for_sentences = top_thread_future.result()\n",
    "    \n",
    "    # Zeroshot Topics\n",
    "    df_doc_zero_topics, zero_topics_for_sentences = zer_thread_future.result()\n",
    "   \n",
    "    # Custom Embeddings\n",
    "    df_adj_embeddings, df_verb_embeddings, df_noun_embeddings = emb_thread_future.result()\n",
    "    \n",
    "    # Markov Pred\n",
    "    df_markov_prediction = mar_thread_future.result()\n",
    "    \n",
    "    # Attribution\n",
    "    attr_labels, df_attr_embedding, df_attr_embedding_weighted, df_attr_embedding_doc2vec, df_attr_embedding_weighted_doc2vec = att_thread_future.result()\n",
    "    \n",
    "    topic_labels = ['CRIME', 'DISSENT', 'DOMESTIC ECONOMIC', 'DOMESTIC POLITICAL', 'ENERGY', 'ENVIRONMENT', 'HEALTH', 'HUMAN RIGHTS', 'INTERNATIONAL ECONOMIC', 'INTERNATIONAL POLITICAL', 'LEADER', 'MEDIA', 'MIGRATION', 'MILITARY', 'NARCOTICS', 'OTHER', 'PEACEKEEPING', 'PROLIFERATION', 'TECHNOLOGY', 'TELECOM', 'TERRORISM', 'TERRORISM.911USA', 'TERRORISM.911WEB', 'URGENT', 'End']\n",
    "    \n",
    "    zero_topic_labels = ['business', 'economy', 'education', 'entertainment', 'environment', 'government', 'health', 'politics', 'sports', 'war', 'End']\n",
    "\n",
    "    Attribution_Labels = ['Real', 'Fake']\n",
    "    Entailment_Labels = ['CONTRADICTION', 'ENTAILMENT', 'NEUTRAL']\n",
    "\n",
    "    Topic_Bigram = list(itertools.permutations(topic_labels, 2))\n",
    "    Topic_Bigram = Topic_Bigram[:-24] + [(elem,) + (elem,) for elem in topic_labels][:-1]\n",
    "    Topic_Attribution_Labels = [elem + ('Real',) for elem in Topic_Bigram] + [elem + ('Fake',) for elem in Topic_Bigram]\n",
    "    Topic_Attribution_Labels.sort()\n",
    "    Topic_Entailment_labels = [elem + ('CONTRADICTION',) for elem in Topic_Bigram] + [elem + ('ENTAILMENT',) for elem in Topic_Bigram]\n",
    "    Topic_Entailment_labels.sort()\n",
    "    Topic_Attribution_Labels_type1 = [(elem,) + ('Real',) for elem in topic_labels] + [(elem,) + ('Fake',) for elem in topic_labels]\n",
    "    Topic_Attribution_Labels_type1.sort()\n",
    "    \n",
    "    Zero_Topic_Bigram = list(itertools.permutations(zero_topic_labels, 2))\n",
    "    Zero_Topic_Bigram = Zero_Topic_Bigram[:-10] + [(elem,) + (elem,) for elem in  zero_topic_labels][:-1]\n",
    "    Zero_Topic_Attribution_Labels = [elem + ('Real',) for elem in Zero_Topic_Bigram] + [elem + ('Fake',) for elem in Zero_Topic_Bigram]\n",
    "    Zero_Topic_Attribution_Labels.sort()\n",
    "    Zero_Topic_Entailment_labels = [elem + ('CONTRADICTION',) for elem in Zero_Topic_Bigram] + [elem + ('ENTAILMENT',) for elem in Zero_Topic_Bigram]\n",
    "    Zero_Topic_Entailment_labels.sort()\n",
    "    Zero_Topic_Attribution_Labels_type1 = [(elem,) + ('Real',) for elem in zero_topic_labels] + [(elem,) + ('Fake',) for elem in zero_topic_labels]\n",
    "    Zero_Topic_Attribution_Labels_type1.sort()\n",
    "    \n",
    "    Attr_bigrams = list(itertools.permutations(Attribution_Labels, 2)) +[(elem,) + (elem,) for elem in Attribution_Labels]\n",
    "    Attr_Entailment_labels = [elem + ('CONTRADICTION',) for elem in Attr_bigrams] + [elem + ('ENTAILMENT',) for elem in Attr_bigrams] + [elem + ('NEUTRAL',) for elem in Attr_bigrams]\n",
    "    Attr_Entailment_labels.sort()\n",
    "    \n",
    "    Topics_Attribution, Topics_Attribution_embedding, Topics_Attribution_type1, Topics_Attribution_embedding_type1, Zero_Topics_Attribution_embedding, Zero_Topics_Attribution_embedding_type1 = x_attribute_feature(\n",
    "            attr_labels, topic_labels, Topic_Attribution_Labels, Topic_Attribution_Labels_type1, topics_for_sentences, zero_topics_for_sentences, zero_topic_labels, Zero_Topic_Attribution_Labels, Zero_Topic_Attribution_Labels_type1)\n",
    "    \n",
    "    Attribution_Entailment, Attr_Entailment_embedding, Topics_Entailment, Topics_Entailment_embedding, Zero_Topics_Entailment_embedding = x_entailment_feature(\n",
    "            attr_labels, entailment_feature_label, Attr_Entailment_labels, topic_labels, Topic_Entailment_labels, topics_for_sentences, zero_topics_for_sentences, zero_topic_labels, Zero_Topic_Entailment_labels)\n",
    "    \n",
    "    df_Topics_Attribution_embedding = pd.DataFrame(Topics_Attribution_embedding).T\n",
    "    df_Topics_Attribution_embedding.columns = Topic_Attribution_Labels\n",
    "    df_Zero_Topics_Attribution_embedding = pd.DataFrame(Zero_Topics_Attribution_embedding).T\n",
    "    df_Zero_Topics_Attribution_embedding.columns = Zero_Topic_Attribution_Labels\n",
    "    \n",
    "    df_Topics_Attribution_embedding_type1 = pd.DataFrame(Topics_Attribution_embedding_type1).T\n",
    "    df_Topics_Attribution_embedding_type1.columns = Topic_Attribution_Labels_type1\n",
    "    df_Zero_Topics_Attribution_embedding_type1 = pd.DataFrame(Zero_Topics_Attribution_embedding_type1).T\n",
    "    df_Zero_Topics_Attribution_embedding_type1.columns = Zero_Topic_Attribution_Labels_type1\n",
    "    \n",
    "    df_Attr_Entailment_embedding = pd.DataFrame(Attr_Entailment_embedding).T\n",
    "    df_Attr_Entailment_embedding.columns = Attr_Entailment_labels\n",
    "    \n",
    "    df_Topics_Entailment_embedding = pd.DataFrame(Topics_Entailment_embedding).T\n",
    "    df_Topics_Entailment_embedding.columns = Topic_Entailment_labels\n",
    "    df_Zero_Topics_Entailment_embedding = pd.DataFrame(Zero_Topics_Entailment_embedding).T\n",
    "    df_Zero_Topics_Entailment_embedding.columns = Zero_Topic_Entailment_labels\n",
    "    \n",
    "    df_Hypernym_Hyponym = Hypernym_Hyponym(chunks)[1]\n",
    "    \n",
    "    df_high_level = pd.concat([df_entailment_type1, df_entailment_type2, df_personality, df_sentiment_type1, df_sentiment_type2, df_raw_doctopic, df_normalised_doctopic, df_adj_embeddings, df_verb_embeddings, df_noun_embeddings, df_markov_prediction, df_attr_embedding, df_attr_embedding_weighted, df_attr_embedding_doc2vec, df_attr_embedding_weighted_doc2vec, df_Topics_Attribution_embedding, df_Topics_Attribution_embedding_type1, df_Attr_Entailment_embedding, df_Topics_Entailment_embedding, df_Hypernym_Hyponym, df_doc_zero_topics, df_Zero_Topics_Attribution_embedding, df_Zero_Topics_Attribution_embedding_type1, df_Zero_Topics_Entailment_embedding], axis=1)\n",
    "    \n",
    "    col_names_final = []\n",
    "    for x in df_high_level.columns:\n",
    "        if type(x) == tuple:\n",
    "            if len(x) == 3:\n",
    "                col_names_final.append('{}_{}_{}'.format(x[0], x[1], x[2]))\n",
    "            else :\n",
    "                col_names_final.append('{}_{}'.format(x[0], x[1]))\n",
    "        else:\n",
    "            col_names_final.append(x)\n",
    "            \n",
    "    df_high_level.columns = col_names_final\n",
    "    \n",
    "    col_names_final = [col.replace(\".\", \"_\") if \".\" in col else col for col in df_high_level.columns]\n",
    "    df_high_level.columns = col_names_final\n",
    "            \n",
    "    df_high_level = df_high_level.loc[:, ~df_high_level.columns.duplicated()]\n",
    "    df_high_level = df_high_level.drop(['Fake_weight', 'Fake_doc2vec_wt'], axis=1)\n",
    "    df_high_level = df_high_level[seq_of_col]\n",
    "    \n",
    "    # Send message\n",
    "    file_locker.acquire_file_lock('tmp/' + str(req_id) + \".pickle\")\n",
    "    stored_requests[req_id][\"message\"] = \"Topic Attribution done...\"\n",
    "    with open('tmp/' + str(req_id) + \".pickle\", \"wb\") as handle:\n",
    "        pickle.dump(stored_requests[req_id], handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    file_locker.delete_file_lock('tmp/' + str(req_id) + \".pickle\")\n",
    "    \n",
    "    return(df_high_level, attr_labels,sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute high-level features zero-shot learning \n",
    "def high_level_features_wo_zeroshot(req_id, text, vec_for_personality, model_for_personality, dberta_model, model_for_topic, models_for_adj, glove_embeddings, trained_markov_real_structure, trained_markov_fake_structure, trained_dberta_embeddings, trained_data, mapping_sent_doc, doc2vec_model, seq_of_col, Entailment_Classifier, Sentiment_Classifier, attr_model):\n",
    "    global file_locker\n",
    "\n",
    "    # Send message\n",
    "    file_locker.acquire_file_lock('tmp/' + str(req_id) + \".pickle\")\n",
    "    stored_requests[req_id][\"message\"] = \"Keep Calm and let the magic of 2nd pipeline take over\"\n",
    "    with open('tmp/' + str(req_id) + \".pickle\", \"wb\") as handle:\n",
    "        pickle.dump(stored_requests[req_id], handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    file_locker.delete_file_lock('tmp/' + str(req_id) + \".pickle\")\n",
    "    \n",
    "    data = text\n",
    "    sentences, sentence_pairs = prepare_sentences_pipeline2(data)\n",
    "    chunks = chunk_sentences(sentences)\n",
    "    \n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=8) as executor:\n",
    "        ent_thread_future = executor.submit(entailment_thread, req_id, sentence_pairs, sentences)\n",
    "        per_thread_future = executor.submit(personality_thread, req_id, data, vec_for_personality, model_for_personality, sentences)\n",
    "        top_thread_future = executor.submit(topic_features_thread, req_id, data, sentences, dberta_model, model_for_topic)\n",
    "        emb_thread_future = executor.submit(custom_emb_thread, req_id, sentences, models_for_adj, glove_embeddings)\n",
    "        mar_thread_future = executor.submit(markov_thread, req_id, sentences, trained_markov_real_structure, trained_markov_fake_structure)\n",
    "        att_thread_future = executor.submit(attribution_thread, req_id, trained_dberta_embeddings, sentences, attr_model, trained_data, doc2vec_model, mapping_sent_doc)\n",
    "        \n",
    "    # Entailments\n",
    "    entailment_feature_label, df_entailment_type1, df_entailment_type2 = ent_thread_future.result()\n",
    "    \n",
    "    # Personality and Sentiment\n",
    "    df_personality, df_sentiment_type1, df_sentiment_type2 = per_thread_future.result()\n",
    "    \n",
    "    # Topics\n",
    "    df_raw_doctopic, df_normalised_doctopic, topics_for_sentences = top_thread_future.result()\n",
    "   \n",
    "    # Custom Embeddings\n",
    "    df_adj_embeddings, df_verb_embeddings, df_noun_embeddings = emb_thread_future.result()\n",
    "    \n",
    "    # Markov Pred\n",
    "    df_markov_prediction = mar_thread_future.result()\n",
    "    \n",
    "    # Attribution\n",
    "    attr_labels, df_attr_embedding, df_attr_embedding_weighted, df_attr_embedding_doc2vec, df_attr_embedding_weighted_doc2vec = att_thread_future.result()\n",
    "    \n",
    "    topic_labels = ['CRIME', 'DISSENT', 'DOMESTIC ECONOMIC', 'DOMESTIC POLITICAL', 'ENERGY', 'ENVIRONMENT', 'HEALTH', 'HUMAN RIGHTS', 'INTERNATIONAL ECONOMIC', 'INTERNATIONAL POLITICAL', 'LEADER', 'MEDIA', 'MIGRATION', 'MILITARY', 'NARCOTICS', 'OTHER', 'PEACEKEEPING', 'PROLIFERATION', 'TECHNOLOGY', 'TELECOM', 'TERRORISM', 'TERRORISM.911USA', 'TERRORISM.911WEB', 'URGENT', 'End']\n",
    "\n",
    "    Attribution_Labels = ['Real', 'Fake']\n",
    "    Entailment_Labels = ['CONTRADICTION', 'ENTAILMENT', 'NEUTRAL']\n",
    "\n",
    "    Topic_Bigram = list(itertools.permutations(topic_labels, 2))\n",
    "    Topic_Bigram = Topic_Bigram[:-24] + [(elem,) + (elem,) for elem in  topic_labels][:-1]\n",
    "    Topic_Attribution_Labels = [elem + ('Real',) for elem in Topic_Bigram] + [ elem + ('Fake',) for elem in Topic_Bigram]\n",
    "    Topic_Attribution_Labels.sort()\n",
    "    Topic_Entailment_labels = [elem + ('CONTRADICTION',) for elem in Topic_Bigram] + [ elem + ('ENTAILMENT',) for elem in Topic_Bigram]\n",
    "    Topic_Entailment_labels.sort()\n",
    "    Topic_Attribution_Labels_type1 = [(elem,) + ('Real',) for elem in topic_labels] + [ (elem,) + ('Fake',) for elem in topic_labels]\n",
    "    Topic_Attribution_Labels_type1.sort()\n",
    "    \n",
    "    Attr_bigrams = list(itertools.permutations(Attribution_Labels,2)) + [(elem,) + (elem,) for elem in  Attribution_Labels]\n",
    "    Attr_Entailment_labels = [elem + ('CONTRADICTION',) for elem in Attr_bigrams] + [ elem + ('ENTAILMENT',) for elem in Attr_bigrams] + [ elem + ('NEUTRAL',) for elem in Attr_bigrams]\n",
    "    Attr_Entailment_labels.sort()\n",
    "    \n",
    "    Topics_Attribution, Topics_Attribution_embedding, Topics_Attribution_type1, Topics_Attribution_embedding_type1 = x_attribute_feature_wo_zeroshot(\n",
    "            attr_labels, topic_labels, Topic_Attribution_Labels, Topic_Attribution_Labels_type1, topics_for_sentences)\n",
    "    \n",
    "    Attribution_Entailment, Attr_Entailment_embedding, Topics_Entailment, Topics_Entailment_embedding = x_entailment_feature_wo_zeroshot(attr_labels, entailment_feature_label, Attr_Entailment_labels, topic_labels, Topic_Entailment_labels, topics_for_sentences)\n",
    "    \n",
    "    df_Topics_Attribution_embedding = pd.DataFrame(Topics_Attribution_embedding).T\n",
    "    df_Topics_Attribution_embedding.columns = Topic_Attribution_Labels\n",
    "\n",
    "    df_Topics_Attribution_embedding_type1 = pd.DataFrame(Topics_Attribution_embedding_type1).T\n",
    "    df_Topics_Attribution_embedding_type1.columns = Topic_Attribution_Labels_type1\n",
    "    \n",
    "    df_Attr_Entailment_embedding = pd.DataFrame(Attr_Entailment_embedding).T\n",
    "    df_Attr_Entailment_embedding.columns = Attr_Entailment_labels\n",
    "    \n",
    "    df_Topics_Entailment_embedding = pd.DataFrame(Topics_Entailment_embedding).T\n",
    "    df_Topics_Entailment_embedding.columns = Topic_Entailment_labels\n",
    "    \n",
    "    df_Hypernym_Hyponym = Hypernym_Hyponym(chunks)[1]\n",
    "    \n",
    "    df_high_level = pd.concat([df_entailment_type1, df_entailment_type2, df_personality, df_sentiment_type1, df_sentiment_type2, df_raw_doctopic, df_normalised_doctopic, df_adj_embeddings, df_verb_embeddings, df_noun_embeddings, df_markov_prediction, df_attr_embedding, df_attr_embedding_weighted, df_attr_embedding_doc2vec, df_attr_embedding_weighted_doc2vec, df_Topics_Attribution_embedding, df_Topics_Attribution_embedding_type1, df_Attr_Entailment_embedding, df_Topics_Entailment_embedding, df_Hypernym_Hyponym], axis=1)\n",
    "    \n",
    "    col_names_final = []\n",
    "    for x in df_high_level.columns:\n",
    "        if type(x) == tuple:\n",
    "            if len(x) == 3:\n",
    "                col_names_final.append('{}_{}_{}'.format(x[0], x[1], x[2]))\n",
    "            else :\n",
    "                col_names_final.append('{}_{}'.format(x[0], x[1]))\n",
    "        else:\n",
    "            col_names_final.append(x)\n",
    "            \n",
    "    df_high_level.columns = col_names_final\n",
    "    \n",
    "    col_names_final = [col.replace(\".\", \"_\") if \".\" in col else col for col in df_high_level.columns]\n",
    "    df_high_level.columns = col_names_final\n",
    "            \n",
    "    df_high_level = df_high_level.loc[:, ~df_high_level.columns.duplicated()]\n",
    "    df_high_level = df_high_level.drop(['Fake_weight', 'Fake_doc2vec_wt'], axis=1)\n",
    "    df_high_level = df_high_level[seq_of_col]\n",
    "    \n",
    "    # Send message\n",
    "    file_locker.acquire_file_lock('tmp/' + str(req_id) + \".pickle\")\n",
    "    stored_requests[req_id][\"message\"] = \"Topic Attribution done...\"\n",
    "    with open('tmp/' + str(req_id) + \".pickle\", \"wb\") as handle:\n",
    "        pickle.dump(stored_requests[req_id], handle, protocol = pickle.HIGHEST_PROTOCOL)\n",
    "    file_locker.delete_file_lock('tmp/' + str(req_id) + \".pickle\")\n",
    "    \n",
    "    return(df_high_level, attr_labels,sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute DataFrame with SHAP contributions\n",
    "def get_contrib_df(shap_base_value, shap_values, X_row, topx=None, cutoff=None, sort='abs', cols=None):\n",
    "\n",
    "    assert isinstance(X_row, pd.DataFrame),\\\n",
    "        'X_row should be a pd.DataFrame! Use X.iloc[[index]]'\n",
    "    assert len(X_row.iloc[[0]].values[0].shape) == 1,\\\n",
    "        \"\"\"X is not the right shape: len(X.values[0]) should be 1. \n",
    "            Try passing X.iloc[[index]]\"\"\" \n",
    "    assert sort in {'abs', 'high-to-low', 'low-to-high', 'importance', None}\n",
    "\n",
    "    # start with the shap_base_value\n",
    "    base_df = pd.DataFrame(\n",
    "        {\n",
    "            'col': ['_BASE'],\n",
    "            'contribution': [shap_base_value],\n",
    "            'value': ['']\n",
    "        })\n",
    "\n",
    "    contrib_df = pd.DataFrame(\n",
    "                    {\n",
    "                        'col': X_row.columns,\n",
    "                        'contribution': shap_values,\n",
    "                        'value': X_row.values[0]\n",
    "                    })\n",
    "\n",
    "    if cols is None:\n",
    "        if cutoff is None and topx is not None:\n",
    "            cutoff = contrib_df.contribution.abs().nlargest(topx).min()\n",
    "        elif cutoff is None and topx is None:\n",
    "            cutoff = 0\n",
    "\n",
    "        display_df = contrib_df[contrib_df.contribution.abs() >= cutoff]\n",
    "        if topx is not None and len(display_df) > topx:\n",
    "            # in case of ties around cutoff\n",
    "            display_df = display_df.reindex(\n",
    "                display_df.contribution.abs().sort_values(ascending=False).index).head(topx)\n",
    "\n",
    "        display_df_neg = display_df[display_df.contribution < 0]\n",
    "        display_df_pos = display_df[display_df.contribution >= 0]\n",
    "\n",
    "        rest_df = (contrib_df[~contrib_df.col.isin(display_df.col.tolist())]\n",
    "            .sum().to_frame().T\n",
    "            .assign(col=\"_REST\", value=\"\"))\n",
    "\n",
    "        # sort the df by absolute value from highest to lowest:\n",
    "        if sort == 'abs':\n",
    "            display_df = display_df.reindex(\n",
    "                                display_df.contribution.abs().sort_values(ascending=False).index)\n",
    "            contrib_df = pd.concat([base_df, display_df, rest_df], ignore_index=True)\n",
    "        if sort == 'high-to-low':\n",
    "            display_df_pos = display_df_pos.reindex(\n",
    "                                display_df_pos.contribution.abs().sort_values(ascending=False).index)\n",
    "            display_df_neg = display_df_neg.reindex(\n",
    "                                display_df_neg.contribution.abs().sort_values().index)\n",
    "            contrib_df = pd.concat([base_df, display_df_pos, rest_df, display_df_neg], ignore_index=True)\n",
    "        if sort == 'low-to-high':\n",
    "            display_df_pos = display_df_pos.reindex(\n",
    "                                display_df_pos.contribution.abs().sort_values().index)\n",
    "            display_df_neg = display_df_neg.reindex(\n",
    "                                display_df_neg.contribution.abs().sort_values(ascending=False).index)\n",
    "            contrib_df = pd.concat([base_df, display_df_neg, rest_df, display_df_pos], ignore_index=True)\n",
    "    else:\n",
    "        display_df = contrib_df[contrib_df.col.isin(cols)].set_index('col').reindex(cols).reset_index()\n",
    "        rest_df = (contrib_df[~contrib_df.col.isin(cols)]\n",
    "                       .sum().to_frame().T\n",
    "                       .assign(col=\"_REST\", value=\"\"))\n",
    "        contrib_df = pd.concat([base_df, display_df, rest_df], ignore_index=True)\n",
    "\n",
    "    # add cumulative contribution from top to bottom (for making bar chart):\n",
    "    contrib_df['cumulative'] = contrib_df.contribution.cumsum()\n",
    "    contrib_df['base']= contrib_df['cumulative'] - contrib_df['contribution']  \n",
    "\n",
    "    pred_df = contrib_df[['contribution']].sum().to_frame().T.assign(\n",
    "            col='_PREDICTION', \n",
    "            value=\"\", \n",
    "            cumulative=lambda df:df.contribution, \n",
    "            base=0)\n",
    "    return pd.concat([contrib_df, pred_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append dict to DataFrame\n",
    "def append_dict_to_df(df, row_dict):\n",
    "    return pd.concat([df, pd.DataFrame([row_dict])],\n",
    "                     ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute DataFrame with summary of SHAP contributions\n",
    "def get_contrib_summary_df(contrib_df, model_output=\"raw\", round=2, units=\"\", na_fill=None):\n",
    " \n",
    "    assert model_output in {'raw', 'probability', 'logodds'}\n",
    "    contrib_summary_df = pd.DataFrame(columns=['Reason', 'Effect'])\n",
    "    \n",
    "    for _, row in contrib_df.iterrows():\n",
    "        if row['col'] == '_BASE':\n",
    "            reason = 'Average of population'\n",
    "            effect = \"\"\n",
    "        elif row['col'] == '_REST':\n",
    "            reason = 'Other features combined'\n",
    "            effect = f\"{'+' if row['contribution'] >= 0 else ''}\"\n",
    "        elif row['col'] == '_PREDICTION':\n",
    "            reason = 'Final prediction'\n",
    "            effect = \"\"\n",
    "        else:\n",
    "            if na_fill is not None and row['value'] == na_fill:\n",
    "                reason = f\"{row['col']} = MISSING\"\n",
    "            else:\n",
    "                reason = f\"{row['col']} = {row['value']}\"\n",
    "\n",
    "            effect = f\"{'+' if row['contribution'] >= 0 else ''}\"\n",
    "        if model_output == \"probability\":\n",
    "            effect += str(np.round(100*row['contribution'], round))+'%'\n",
    "        elif model_output == 'logodds':\n",
    "            effect += str(np.round(row['contribution'], round))    \n",
    "        else:\n",
    "            effect +=  str(np.round(row['contribution'], round)) + f\" {units}\"\n",
    "\n",
    "        contrib_summary_df = append_dict_to_df(contrib_summary_df,\n",
    "            dict(Reason=reason, Effect=effect))\n",
    "    \n",
    "    return contrib_summary_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction Pipeline complete\n",
    "def prediction_pipeline_complete(\n",
    "    model_ll, text, vec,model_hl,vec_for_personality, model_for_personality, dberta_model, model_for_topic, models_for_adj, glove_embeddings, trained_markov_real_structure, trained_markov_fake_structure, trained_dberta_embeddings, trained_data, mapping_sent_doc, doc2vec_model, seq_of_col, Entailment_Classifier, Sentiment_Classifier, attr_model, zero_classifier, fake_threshold=0.183, real_threshold=0.841, headline=None):\n",
    "    \n",
    "    features, ftext, ftitle = low_level_features(text, vec, headline)\n",
    "    prediction = model_ll.predict(features)\n",
    "    prediction_proba = model_ll.predict_proba(features)[:,1]\n",
    "    if prediction_proba >= real_threshold:\n",
    "        output = \"Well Well, the article you wanted to reveal was a cheap muggle trick and is handled by the 'Misuse of Muggle Artefacts Office'. A simple revelio charm did the trick and this article is Real with \" + str(round(prediction_proba[0] * 100,2)) + \" % confidence\"\n",
    "    elif prediction_proba <= fake_threshold:\n",
    "        output = \"Well Well, the article you wanted to reveal was a cheap muggle trick and is handled by the 'Misuse of Muggle Artefacts Office'. A simple revelio charm did the trick and this article is Fake with \" + str(round((1-prediction_proba[0])* 100,2)) + \" % confidence\"\n",
    "    else:\n",
    "        prediction_proba = model_hl.predict_proba(high_level_features(text, vec_for_personality, model_for_personality, dberta_model, model_for_topic, models_for_adj, glove_embeddings, trained_markov_real_structure, trained_markov_fake_structure, trained_dberta_embeddings, trained_data, mapping_sent_doc, doc2vec_model, seq_of_col, Entailment_Classifier, Sentiment_Classifier, attr_model, zero_classifier))[:,1]\n",
    "        if prediction_proba[0] >= 0.5:\n",
    "            output = \"We are in Pipeline-2 and This News is Real with \" + str(round(prediction_proba[0] * 100,2)) + \" % confidence\"\n",
    "        else:\n",
    "            output = \"We are in Pipeline-2 and This News is Fake with \" + str(round((1-prediction_proba[0])* 100,2)) + \" % confidence\"\n",
    "    \n",
    "    return (output, ftext, ftitle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction Pipeline-1\n",
    "def prediction_pipeline_1(model_ll, text, vec, fake_threshold=0.183, real_threshold=0.841, headline=None):\n",
    "    features, ftext, ftitle = low_level_features(text, vec, headline)\n",
    "    prediction = model_ll.predict(features)\n",
    "    prediction_proba = model_ll.predict_proba(features)[:,1]\n",
    "    status = 0\n",
    "    if prediction_proba >= real_threshold:\n",
    "        output = \"Well Well, the article you wanted to reveal was a cheap muggle trick and is handled by the 'Misuse of Muggle Artefacts Office'. A simple revelio charm did the trick and this article is Real with \" + str(round(prediction_proba[0] * 100,2)) + \" % confidence\"\n",
    "        status = 1\n",
    "    elif prediction_proba <= fake_threshold:\n",
    "        output = \"Well Well, the article you wanted to reveal was a cheap muggle trick and is handled by the 'Misuse of Muggle Artefacts Office'. A simple revelio charm did the trick and this article is Fake with \" + str(round((1-prediction_proba[0])* 100,2)) + \" % confidence\"\n",
    "        status = -1\n",
    "    else:\n",
    "        output = \"The prediction is not very confident, but we assume it is \"\n",
    "        label = \"Real\"\n",
    "        status = 2\n",
    "        prediction_number = prediction_proba[0]\n",
    "        if prediction[0] == 0:\n",
    "            label = \"Fake\"\n",
    "            status = -2\n",
    "            prediction_number = 1-prediction_number\n",
    "        output += label + \" with \" + str(round(prediction_number* 100,2)) + \" % confidence\"\n",
    "\n",
    "    return (output, ftext, ftitle, status, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute contribution explanations\n",
    "def get_contribution_explain(model_hl, features):\n",
    "    explainer = shap.TreeExplainer(model_hl)\n",
    "    shap_values = explainer.shap_values(features)\n",
    "    shap_obj = explainer(features)\n",
    "    contrib_df = get_contrib_df(explainer.expected_value, shap_values[0], features, topx=None, cutoff=None, sort='abs', cols=None)\n",
    "    df_explain = get_contrib_summary_df(contrib_df, model_output=\"raw\", round=2, units=\"\", na_fill=None)\n",
    "    \n",
    "    return df_explain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute figure arrow size\n",
    "def figure_arrow_size(value, arrow_cuts):\n",
    "    abs_value = abs(value)\n",
    "    arrow_size = len(arrow_cuts)\n",
    "    for i in range(len(arrow_cuts)):\n",
    "        if arrow_cuts[i] > abs_value:\n",
    "            arrow_size = len(arrow_cuts) - i\n",
    "    if value < 0:\n",
    "        return -1 * arrow_size\n",
    "    else:\n",
    "        return arrow_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction Pipeline-1 child\n",
    "def predict_pipeline1_child(req_id, text, title):\n",
    "    global file_locker\n",
    "\n",
    "    prediction, ftext, ftitle, status, ll_feat = prediction_pipeline_1(model_ll, text, tf_idf_vectorizer_ll, fake_threshold=0.183, real_threshold=0.841, headline=title)\n",
    "    # Create arrows from feature contributions\n",
    "    df_explain = get_contribution_explain(model_ll, ll_feat)\n",
    "    max_explain = 0\n",
    "    min_explain = 1000\n",
    "    for i in range(1,5):\n",
    "        value = float(df_explain.iloc[i][\"Effect\"])\n",
    "        abs_expl = abs(value)\n",
    "        if abs_expl > max_explain:\n",
    "            max_explain = abs_expl\n",
    "        if abs_expl < min_explain:\n",
    "            min_explain = abs_expl\n",
    "    one_third = (max_explain-min_explain) / 3.0\n",
    "    arrow_cuts = [max_explain, min_explain + 2*one_third, min_explain + one_third]\n",
    "    explanation_arrows = []\n",
    "    for i in range(1,5):\n",
    "        value = float(df_explain.iloc[i][\"Effect\"])\n",
    "        split_reason = df_explain.iloc[i][\"Reason\"].split(\" = \")\n",
    "        name_expl = split_reason[0]\n",
    "        expl_expl = \"No explanation given\"\n",
    "        number_expl = split_reason[1]\n",
    "        number_expl = round(float(number_expl),3)\n",
    "        if name_expl in explain_dict_ll:\n",
    "            name_expl, expl_expl = explain_dict_ll[name_expl]\n",
    "        else:\n",
    "            expl_expl = \"The tf-idf of the word \" + name_expl + \". tf-idf is the count of the word in relation to their total appearance in all news. A high value indicates a word, that appears often inside the text, but not often in other news. It denotes therefore an importance for distinguishing articles from each other.\"\n",
    "        explanation_arrows.append((name_expl, number_expl, figure_arrow_size(value, arrow_cuts), expl_expl))\n",
    "    file_locker.acquire_file_lock('tmp/' + str(req_id) + \".pickle\")\n",
    "    stored_requests[req_id][\"explanation\"] = explanation_arrows\n",
    "    stored_requests[req_id][\"prediction\"] = prediction\n",
    "    stored_requests[req_id][\"text\"] = ftext\n",
    "    stored_requests[req_id][\"title\"] = ftitle\n",
    "    stored_requests[req_id][\"status\"] = status\n",
    "    stored_requests[req_id][\"message\"] = \"Pipeline 1 has finished.\"\n",
    "    sent = sent_tokenize(ftext)\n",
    "    a_third = int(len(sent) / 2.25)\n",
    "    sec_third = int((len(sent) - a_third) / 2) + a_third\n",
    "    split_text = [\" \".join(sent[0:a_third]), \" \".join(sent[a_third:sec_third]), \" \".join(sent[sec_third:len(sent)])]\n",
    "    stored_requests[req_id][\"split_info\"] = [a_third, sec_third, len(sent)]\n",
    "    stored_requests[req_id][\"split_text\"] = split_text\n",
    "    stored_requests[req_id][\"quote\"] = sent[a_third]\n",
    "    with open('tmp/' + str(req_id) + \".pickle\", \"wb\") as handle:\n",
    "        pickle.dump(stored_requests[req_id], handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    file_locker.delete_file_lock('tmp/' + str(req_id) + \".pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wizard explainability\n",
    "def wizard_explainability(model_explain, features, col_explain=None):\n",
    "    if col_explain:\n",
    "        explainer = ClassifierExplainer(model_explain, features[col_explain],model_output='probability')\n",
    "    else:\n",
    "        explainer = ClassifierExplainer(model_explain, features, model_output='probability')\n",
    "    db = ExplainerDashboard(explainer, title=\"FACADE\", contributions=False,hide_whatifpdp=True, bootstrap=dbc.themes.SKETCHY)\n",
    "    db.run(mode=\"external\", port=8052)\n",
    "    return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Highlighting sentences data\n",
    "def highlighting_sentences_data(Attribution_labels, trained_data, all_sentences_training, sentences_query):\n",
    "    labels_to_use = Attribution_labels\n",
    "    Sentence_Weight_Dataset = []\n",
    "    for i in range(len(labels_to_use)):\n",
    "        Sentence_Weight_Dataset.append((sentences_query[i], labels_to_use[i]))\n",
    "        \n",
    "    df_coeff = pd.DataFrame(Sentence_Weight_Dataset)\n",
    "    df_coeff.columns = ['word', 'num_code']\n",
    "    df_coeff['label'] = [elem[1] for elem in df_coeff['num_code']]\n",
    "    df_coeff['Source'] = [elem[2] for elem in df_coeff['num_code']]\n",
    "    df_coeff['num_code'] = [elem[0] for elem in df_coeff['num_code']]\n",
    "    \n",
    "    if len(df_coeff[df_coeff['label'] == \"Real\"]['num_code']) < 2:\n",
    "        max_alpha_real = 1\n",
    "    else:\n",
    "        max_alpha_real = max(df_coeff[df_coeff['label'] == \"Real\"]['num_code'])\n",
    "   \n",
    "    if len(df_coeff[df_coeff['label'] == \"Fake\"]['num_code']) < 2:\n",
    "        max_alpha_fake = 1\n",
    "    else:\n",
    "        max_alpha_fake = max(df_coeff[df_coeff['label'] == \"Fake\"]['num_code'])\n",
    "        \n",
    "    if len(df_coeff[df_coeff['label'] == \"Real\"]['num_code']) < 2:\n",
    "        min_alpha_real = 0\n",
    "    else:\n",
    "        min_alpha_real = min(df_coeff[df_coeff['label'] == \"Real\"]['num_code'])\n",
    "   \n",
    "    if len(df_coeff[df_coeff['label'] == \"Fake\"]['num_code']) < 2:\n",
    "        min_alpha_fake = 0\n",
    "    else:\n",
    "        min_alpha_fake = min(df_coeff[df_coeff['label'] == \"Fake\"]['num_code'])\n",
    "    \n",
    "    highlighted_text = []\n",
    "    for i in range(len(df_coeff)):\n",
    "        word = df_coeff['word'][i]\n",
    "        weight = df_coeff['num_code'][i]\n",
    "        label = df_coeff['label'] [i]\n",
    "        if label != 'Unknown':\n",
    "            Domain_of_Source = trained_data['domain'][ df_coeff['Source'][i][0]]\n",
    "            Title_of_Source =  trained_data['title'][ df_coeff['Source'][i][0]]\n",
    "            Sentence = all_sentences_training[df_coeff['Source'][i][0]][df_coeff['Source'][i][1]]\n",
    "            Sentence = html.escape(Sentence)\n",
    "            min_alpha = 0\n",
    "            max_alpha = 0\n",
    "            if label == 'Real':\n",
    "                min_alpha = min_alpha_real\n",
    "                max_alpha = max_alpha_real\n",
    "            else:\n",
    "                min_alpha = min_alpha_fake\n",
    "                max_alpha = max_alpha_fake\n",
    "            \n",
    "            highlighted_text.append((word, label, weight, Domain_of_Source, Title_of_Source, Sentence, max_alpha, min_alpha))\n",
    "        else:\n",
    "            highlighted_text.append((word, \"Unknown\"))\n",
    "        \n",
    "    return(highlighted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction Pipeline-2\n",
    "def prediction_pipeline_2(req_id, model_hl, text, vec_for_personality, model_for_personality, dberta_model, model_for_topic, models_for_adj, glove_embeddings, trained_markov_real_structure, trained_markov_fake_structure, trained_dberta_embeddings, trained_data, mapping_sent_doc, doc2vec_model, seq_of_col, Entailment_Classifier, Sentiment_Classifier, attr_model, zero_classifier=None):\n",
    "    global file_locker\n",
    "    global running_db\n",
    "\n",
    "    if zero_classifier:\n",
    "        features, attr_labels, sentences_query = high_level_features(req_id, text, vec_for_personality, model_for_personality, dberta_model, model_for_topic, models_for_adj, glove_embeddings, trained_markov_real_structure, trained_markov_fake_structure, trained_dberta_embeddings, trained_data, mapping_sent_doc, doc2vec_model, seq_of_col, Entailment_Classifier, Sentiment_Classifier, attr_model, zero_classifier)\n",
    "    else:\n",
    "        features, attr_labels, sentences_query = high_level_features_wo_zeroshot(req_id, text, vec_for_personality, model_for_personality, dberta_model, model_for_topic, models_for_adj, glove_embeddings, trained_markov_real_structure, trained_markov_fake_structure, trained_dberta_embeddings, trained_data, mapping_sent_doc, doc2vec_model, seq_of_col, Entailment_Classifier, Sentiment_Classifier, attr_model)\n",
    "    prediction_proba = model_hl.predict_proba(features)[:,1]\n",
    "    print(features.columns)\n",
    "    status = 0\n",
    "    if prediction_proba[0] >= 0.5:\n",
    "        output = \"We are in Pipeline-2 and This News is Real with \" + str(round(prediction_proba[0] * 100,2)) + \" % confidence\"\n",
    "        status = 1\n",
    "    else:\n",
    "        output = \"We are in Pipeline-2 and This News is Fake with \" + str(round((1-prediction_proba[0])* 100,2)) + \" % confidence\"\n",
    "        status = -1\n",
    "    \n",
    "    if zero_classifier:\n",
    "        running_db = wizard_explainability(model_explain, features, col_explain)\n",
    "    else:\n",
    "        print(\"No wizardry without zero shot at the moment!\")\n",
    "    \n",
    "    highlighted_text = highlighting_sentences_data(attr_labels, trained_data, all_sentences_training, sentences_query)\n",
    "\n",
    "    # Send message\n",
    "    file_locker.acquire_file_lock('tmp/' + str(req_id) + \".pickle\")\n",
    "    stored_requests[req_id][\"message\"] = \"Generated highlights...\"\n",
    "    with open('tmp/' + str(req_id) + \".pickle\", \"wb\") as handle:\n",
    "        pickle.dump(stored_requests[req_id], handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    file_locker.delete_file_lock('tmp/' + str(req_id) + \".pickle\")\n",
    "    \n",
    "    # Create arrows from feature contributions\n",
    "    df_explain = get_contribution_explain(model_hl, features)\n",
    "    max_explain = 0\n",
    "    min_explain = 1000\n",
    "    for i in range(1,5):\n",
    "        value = float(df_explain.iloc[i][\"Effect\"])\n",
    "        abs_expl = abs(value)\n",
    "        if abs_expl > max_explain:\n",
    "            max_explain = abs_expl\n",
    "        if abs_expl < min_explain:\n",
    "            min_explain = abs_expl\n",
    "    one_third = (max_explain-min_explain) / 3.0\n",
    "    arrow_cuts = [max_explain, min_explain + 2*one_third, min_explain + one_third]\n",
    "    explanation_arrows = []\n",
    "    for i in range(1,5):\n",
    "        value = float(df_explain.iloc[i][\"Effect\"])\n",
    "        split_reason = df_explain.iloc[i][\"Reason\"].split(\" = \")\n",
    "        name_expl = split_reason[0]\n",
    "        expl_expl = \"No explanation given\"\n",
    "        number_expl = split_reason[1]\n",
    "        number_expl = round(float(number_expl),3)\n",
    "        if name_expl in explain_dict_hl:\n",
    "            name_expl,expl_expl = explain_dict_hl[name_expl]\n",
    "        else:\n",
    "            expl_expl = \"No explanation given, yet.\"\n",
    "        explanation_arrows.append((name_expl, number_expl, figure_arrow_size(value, arrow_cuts), expl_expl))\n",
    "    \n",
    "    file_locker.acquire_file_lock('tmp/' + str(req_id) + \".pickle\")\n",
    "    stored_requests[req_id][\"attribution\"] = highlighted_text\n",
    "    stored_requests[req_id][\"prediction\"] = output\n",
    "    a_third = int(len(sentences_query) / 2.25)\n",
    "    sec_third = int((len(sentences_query) - a_third) / 2) + a_third\n",
    "    stored_requests[req_id][\"split_info\"] = [a_third, sec_third, len(sentences_query)]\n",
    "    stored_requests[req_id][\"status\"] = status\n",
    "    stored_requests[req_id][\"wizardry\"] = zero_classifier != None\n",
    "    stored_requests[req_id][\"message\"] = \"Pipeline 2 has finished.\"\n",
    "        \n",
    "    stored_requests[req_id][\"explanation\"] = explanation_arrows\n",
    "    \n",
    "    with open('tmp/' + str(req_id) + \".pickle\", \"wb\") as handle:\n",
    "        pickle.dump(stored_requests[req_id], handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    file_locker.delete_file_lock('tmp/' + str(req_id) + \".pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flask App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__' (lazy loading)\n",
      " * Environment: production\n",
      "   WARNING: This is a development server. Do not use it in a production deployment.\n",
      "   Use a production WSGI server instead.\n",
      " * Debug mode: on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n",
      "127.0.0.1 - - [22/Oct/2022 05:35:41] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [22/Oct/2022 05:35:41] \"\u001b[36mGET /assets/web/assets/mobirise-icons-bold/mobirise-icons-bold.css HTTP/1.1\u001b[0m\" 304 -\n",
      "127.0.0.1 - - [22/Oct/2022 05:35:41] \"\u001b[36mGET /assets/web/assets/mobirise-icons/mobirise-icons.css HTTP/1.1\u001b[0m\" 304 -\n",
      "127.0.0.1 - - [22/Oct/2022 05:35:41] \"\u001b[36mGET /assets/web/assets/mobirise-icons2/mobirise2.css HTTP/1.1\u001b[0m\" 304 -\n",
      "127.0.0.1 - - [22/Oct/2022 05:35:41] \"\u001b[36mGET /assets/bootstrap/css/bootstrap.min.css HTTP/1.1\u001b[0m\" 304 -\n",
      "127.0.0.1 - - [22/Oct/2022 05:35:41] \"\u001b[36mGET /assets/bootstrap/css/bootstrap-grid.min.css HTTP/1.1\u001b[0m\" 304 -\n",
      "127.0.0.1 - - [22/Oct/2022 05:35:41] \"\u001b[36mGET /assets/bootstrap/css/bootstrap-reboot.min.css HTTP/1.1\u001b[0m\" 304 -\n",
      "127.0.0.1 - - [22/Oct/2022 05:35:41] \"\u001b[36mGET /assets/parallax/jarallax.css HTTP/1.1\u001b[0m\" 304 -\n",
      "127.0.0.1 - - [22/Oct/2022 05:35:41] \"\u001b[36mGET /assets/animatecss/animate.css HTTP/1.1\u001b[0m\" 304 -\n",
      "127.0.0.1 - - [22/Oct/2022 05:35:42] \"\u001b[36mGET /assets/dropdown/css/style.css HTTP/1.1\u001b[0m\" 304 -\n",
      "127.0.0.1 - - [22/Oct/2022 05:35:42] \"\u001b[36mGET /assets/socicon/css/styles.css HTTP/1.1\u001b[0m\" 304 -\n",
      "127.0.0.1 - - [22/Oct/2022 05:35:42] \"\u001b[36mGET /assets/theme/css/style.css HTTP/1.1\u001b[0m\" 304 -\n",
      "127.0.0.1 - - [22/Oct/2022 05:35:42] \"\u001b[36mGET /assets/waitMe/waitMe.min.css HTTP/1.1\u001b[0m\" 304 -\n",
      "127.0.0.1 - - [22/Oct/2022 05:35:42] \"\u001b[36mGET /assets/daily_prophet/daily_prophet.css HTTP/1.1\u001b[0m\" 304 -\n",
      "127.0.0.1 - - [22/Oct/2022 05:35:42] \"\u001b[36mGET /assets/jquery-ui.min.css HTTP/1.1\u001b[0m\" 304 -\n",
      "127.0.0.1 - - [22/Oct/2022 05:35:42] \"\u001b[36mGET /assets/mobirise/css/mbr-additional.css HTTP/1.1\u001b[0m\" 304 -\n",
      "127.0.0.1 - - [22/Oct/2022 05:35:42] \"\u001b[36mGET /assets/fonts/style.css HTTP/1.1\u001b[0m\" 304 -\n",
      "127.0.0.1 - - [22/Oct/2022 05:35:42] \"\u001b[36mGET /assets/main.css HTTP/1.1\u001b[0m\" 304 -\n",
      "127.0.0.1 - - [22/Oct/2022 05:35:42] \"\u001b[36mGET /assets/jquery-3.6.0.min.js HTTP/1.1\u001b[0m\" 304 -\n",
      "127.0.0.1 - - [22/Oct/2022 05:35:42] \"\u001b[36mGET /assets/bootstrap/js/bootstrap.bundle.min.js HTTP/1.1\u001b[0m\" 304 -\n",
      "127.0.0.1 - - [22/Oct/2022 05:35:42] \"\u001b[36mGET /assets/jquery-ui.min.js HTTP/1.1\u001b[0m\" 304 -\n",
      "127.0.0.1 - - [22/Oct/2022 05:35:42] \"\u001b[36mGET /assets/parallax/jarallax.js HTTP/1.1\u001b[0m\" 304 -\n",
      "127.0.0.1 - - [22/Oct/2022 05:35:42] \"\u001b[36mGET /assets/smoothscroll/smooth-scroll.js HTTP/1.1\u001b[0m\" 304 -\n",
      "127.0.0.1 - - [22/Oct/2022 05:35:42] \"\u001b[36mGET /assets/dropdown/js/navbar-dropdown.js HTTP/1.1\u001b[0m\" 304 -\n",
      "127.0.0.1 - - [22/Oct/2022 05:35:42] \"\u001b[36mGET /assets/ytplayer/index.js HTTP/1.1\u001b[0m\" 304 -\n",
      "127.0.0.1 - - [22/Oct/2022 05:35:42] \"\u001b[36mGET /assets/theme/js/script.js HTTP/1.1\u001b[0m\" 304 -\n",
      "127.0.0.1 - - [22/Oct/2022 05:35:42] \"\u001b[36mGET /assets/waitMe/waitMe.min.js HTTP/1.1\u001b[0m\" 304 -\n",
      "127.0.0.1 - - [22/Oct/2022 05:35:42] \"\u001b[36mGET /assets/main.js HTTP/1.1\u001b[0m\" 304 -\n",
      "127.0.0.1 - - [22/Oct/2022 05:35:43] \"\u001b[36mGET /assets/images/release-1-platform-9-3-4-1920x1080.png HTTP/1.1\u001b[0m\" 304 -\n",
      "127.0.0.1 - - [22/Oct/2022 05:35:43] \"\u001b[36mGET /assets/socicon/fonts/socicon.woff2 HTTP/1.1\u001b[0m\" 304 -\n",
      "127.0.0.1 - - [22/Oct/2022 05:35:43] \"\u001b[36mGET /assets/images/mbr-1920x1496.jpg HTTP/1.1\u001b[0m\" 304 -\n",
      "127.0.0.1 - - [22/Oct/2022 05:35:43] \"\u001b[36mGET /assets/web/assets/mobirise-icons-bold/mobirise-icons-bold.ttf?m1l4yr HTTP/1.1\u001b[0m\" 304 -\n",
      "127.0.0.1 - - [22/Oct/2022 05:35:43] \"\u001b[36mGET /assets/fonts/HarryP-MVZ6w/font.ttf HTTP/1.1\u001b[0m\" 304 -\n",
      "127.0.0.1 - - [22/Oct/2022 05:35:44] \"\u001b[36mGET /assets/images/fake-news-on-mobile-phone-line-icon-hoax-fake-false-on-smartphone-linear-pictogram-message-with-misinformation-outline-icon-disinformati-193x193.jpg HTTP/1.1\u001b[0m\" 304 -\n",
      "127.0.0.1 - - [22/Oct/2022 05:35:52] \"GET /predict1/?text=https%3A%2F%2Fwww.bbc.com%2Fnews%2Fhealth-55045639&title= HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [22/Oct/2022 05:35:53] \"\u001b[32mGET /predict_status?request_id=2034564 HTTP/1.1\u001b[0m\" 308 -\n",
      "127.0.0.1 - - [22/Oct/2022 05:35:56] \"GET /predict_status/?request_id=2034564 HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [22/Oct/2022 05:35:59] \"GET /predict_status/?request_id=2034564 HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [22/Oct/2022 05:36:03] \"GET /predict_status/?request_id=2034564 HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [22/Oct/2022 05:36:06] \"GET /predict_status/?request_id=2034564 HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [22/Oct/2022 05:36:10] \"GET /predict_status/?request_id=2034564 HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [22/Oct/2022 05:36:14] \"GET /predict_status/?request_id=2034564 HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [22/Oct/2022 05:36:17] \"GET /predict_status/?request_id=2034564 HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [22/Oct/2022 05:36:19] \"GET /predict_status/?request_id=2034564 HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [22/Oct/2022 05:36:20] \"\u001b[36mGET /assets/images/dumbledore-no.gif HTTP/1.1\u001b[0m\" 304 -\n",
      "127.0.0.1 - - [22/Oct/2022 05:36:22] \"\u001b[32mGET /predict2?request_id=2034564&type=0 HTTP/1.1\u001b[0m\" 308 -\n",
      "127.0.0.1 - - [22/Oct/2022 05:36:23] \"GET /predict2/?request_id=2034564&type=0 HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [22/Oct/2022 05:36:25] \"GET /predict_status/?request_id=2034564 HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [22/Oct/2022 05:36:30] \"GET /predict_status/?request_id=2034564 HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [22/Oct/2022 05:36:50] \"GET /predict_status/?request_id=2034564 HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [22/Oct/2022 05:36:56] \"GET /predict_status/?request_id=2034564 HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [22/Oct/2022 05:41:00] \"GET /predict_status/?request_id=2034564 HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [22/Oct/2022 05:41:03] \"GET /predict_status/?request_id=2034564 HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [22/Oct/2022 05:41:08] \"GET /predict_status/?request_id=2034564 HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [22/Oct/2022 05:41:12] \"GET /predict_status/?request_id=2034564 HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [22/Oct/2022 05:41:16] \"GET /predict_status/?request_id=2034564 HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['CONTRADICTION', 'NEUTRAL', 'ENTAILMENT', 'CONTRADICTION_norm',\n",
      "       'NEUTRAL_norm', 'ENTAILMENT_norm', 'Extrovert', 'Sensing', 'Feeling',\n",
      "       'Perceiving',\n",
      "       ...\n",
      "       'war_government_CONTRADICTION', 'war_government_ENTAILMENT',\n",
      "       'war_health_CONTRADICTION', 'war_health_ENTAILMENT',\n",
      "       'war_politics_CONTRADICTION', 'war_politics_ENTAILMENT',\n",
      "       'war_sports_CONTRADICTION', 'war_sports_ENTAILMENT',\n",
      "       'war_war_CONTRADICTION', 'war_war_ENTAILMENT'],\n",
      "      dtype='object', length=3061)\n",
      "Warning: Detected the following categorical columns: ['economy', 'sports', 'entertainment', 'environment']. Unfortunately for now shap interaction values do not work withcategorical columns.\n",
      "Warning: Models that deal with categorical features directly such as GradientBoostingClassifier are incompatible with model_output='probability' for now. So setting model_output='logodds'...\n",
      "Generating self.shap_explainer = shap.TreeExplainer(model)\n",
      "Building ExplainerDashboard..\n",
      "Detected notebook environment, consider setting mode='external', mode='inline' or mode='jupyterlab' to keep the notebook interactive while the dashboard is running...\n",
      "No y labels were passed to the Explainer, so setting model_summary=False...\n",
      "For this type of model and model_output interactions don't work, so setting shap_interaction=False...\n",
      "The explainer object has no decision_trees property. so setting decision_trees=False...\n",
      "Generating layout...\n",
      "Calculating shap values...\n",
      "Calculating dependencies...\n",
      "Reminder: you can store the explainer (including calculated dependencies) with explainer.dump('explainer.joblib') and reload with e.g. ClassifierExplainer.from_file('explainer.joblib')\n",
      "Registering callbacks...\n",
      "Warning: Original ExplainerDashboard was initialized with mode='dash'. Rebuilding dashboard before launch:\n",
      "Building ExplainerDashboard..\n",
      "No y labels were passed to the Explainer, so setting model_summary=False...\n",
      "For this type of model and model_output interactions don't work, so setting shap_interaction=False...\n",
      "The explainer object has no decision_trees property. so setting decision_trees=False...\n",
      "Generating layout...\n",
      "Calculating dependencies...\n",
      "Reminder: you can store the explainer (including calculated dependencies) with explainer.dump('explainer.joblib') and reload with e.g. ClassifierExplainer.from_file('explainer.joblib')\n",
      "Registering callbacks...\n",
      "Starting ExplainerDashboard on http://192.168.0.20:8052\n",
      "You can terminate the dashboard with ExplainerDashboard.terminate(8052)\n",
      "Dash app running on http://127.0.0.1:8052/\n"
     ]
    }
   ],
   "source": [
    "# Flask App\n",
    "app = Flask(__name__, static_url_path='/assets')\n",
    "app.secret_key = secrets.token_bytes(32)\n",
    "\n",
    "\n",
    "# Flask routes\n",
    "@app.route('/', methods=['GET'])\n",
    "def home():\n",
    "    return render_template('index.html')\n",
    "\n",
    "\n",
    "@app.route('/', methods=['POST'])\n",
    "def webapp():\n",
    "    text = request.form['text']\n",
    "    title = request.form['title']\n",
    "    prediction = prediction_pipeline_complete(\n",
    "        model_ll, text, tf_idf_vectorizer_ll, model_hl, vec_for_personality, model_for_personality, dberta_model, model_for_topic, models_for_adj, glove_embeddings, trained_markov_real_structure, trained_markov_fake_structure, trained_dberta_embeddings, trained_data, mapping_sent_doc, doc2vec_model, seq_of_col, Entailment_Classifier, Sentiment_Classifier, attr_model, zero_classifier, fake_threshold=0.183, real_threshold=0.841, headline=title)\n",
    "\n",
    "    return render_template('index.html', text=text, result=prediction)\n",
    "\n",
    "\n",
    "@app.route('/predict1/', methods=['GET', 'POST'])\n",
    "def predict_pipeline1():\n",
    "    check_for_old_requests()\n",
    "    text = request.args.get(\"text\")\n",
    "    title = request.args.get(\"title\")\n",
    "    req_id = random.randrange(3000000)\n",
    "    stored_requests[req_id] = { \"time\" : time.time(), \"status\": 0, \"text\": text, \"title\": title, \"message\": \"Stay put for Pipeline 1 to finish.\" }\n",
    "    p = threading.Thread(target=predict_pipeline1_child, args=(req_id, text, title,))\n",
    "    p.start()\n",
    "\n",
    "    return jsonify(request_id=req_id, response=stored_requests[req_id])\n",
    "\n",
    "\n",
    "@app.route('/predict2/', methods=['GET', 'POST'])\n",
    "def predict_pipeline2():\n",
    "    check_for_old_requests()\n",
    "    if running_db:\n",
    "        running_db.terminate(8052)\n",
    "    req_id = int(request.args.get(\"request_id\"))\n",
    "    type_id = 0\n",
    "    if request.args.get(\"type\"):\n",
    "        type_id = int(request.args.get(\"type\"))\n",
    "    stored_requests[req_id][\"time\"] = time.time()\n",
    "    stored_requests[req_id][\"status\"] = 0\n",
    "    stored_requests[req_id][\"message\"] = \"Pipeline 2 will take over, so keep calm.\"\n",
    "    \n",
    "    if type_id == 0:\n",
    "        p = threading.Thread(target=prediction_pipeline_2, args=(req_id, model_hl, stored_requests[req_id][\"text\"], vec_for_personality, model_for_personality, dberta_model, model_for_topic, models_for_adj, glove_embeddings, trained_markov_real_structure, trained_markov_fake_structure, trained_dberta_embeddings, trained_data, mapping_sent_doc, doc2vec_model, seq_of_col, Entailment_Classifier, Sentiment_Classifier, attr_model, zero_classifier))\n",
    "    else:\n",
    "        p = threading.Thread(target=prediction_pipeline_2, args=(req_id, model_hl_wo_zeroshot, stored_requests[req_id][\"text\"], vec_for_personality, model_for_personality, dberta_model, model_for_topic, models_for_adj, glove_embeddings, trained_markov_real_structure, trained_markov_fake_structure, trained_dberta_embeddings, trained_data, mapping_sent_doc, doc2vec_model, seq_of_col_wo_zeroshot, Entailment_Classifier, Sentiment_Classifier, attr_model, None))\n",
    "    p.start()\n",
    "    \n",
    "    return jsonify(request_id=req_id, response=stored_requests[req_id])\n",
    "\n",
    "\n",
    "@app.route('/predict_status/', methods=['GET', 'POST'])\n",
    "def predict_status():\n",
    "    global file_locker\n",
    "\n",
    "    req_id = int(request.args.get(\"request_id\"))\n",
    "    if req_id not in stored_requests.keys():\n",
    "        return jsonify(request_id=req_id, response={\"status\":404}, timeout=False)\n",
    "    max_timeout = 2.0\n",
    "    if (request.args.get(\"timeout\")):\n",
    "        max_timeout = float(request.args.get(\"timeout\"))\n",
    "    timeout = 0\n",
    "    while not os.path.exists('tmp/' + str(req_id) + \".pickle\"):\n",
    "        if timeout > max_timeout:\n",
    "            return jsonify(request_id=req_id, response=stored_requests[req_id], timeout=True)\n",
    "        time.sleep(0.25)\n",
    "        timeout += 0.25\n",
    "    file_locker.acquire_file_lock('tmp/' + str(req_id) + \".pickle\")\n",
    "    with open('tmp/' + str(req_id) + \".pickle\", \"rb\") as handle:\n",
    "        keyed_request = pickle.load(handle)\n",
    "    os.remove('tmp/' + str(req_id) + \".pickle\")\n",
    "    time.sleep(0.25)\n",
    "    file_locker.delete_file_lock('tmp/' + str(req_id) + \".pickle\")\n",
    "    stored_requests[req_id] = keyed_request\n",
    "    \n",
    "    return jsonify(request_id=req_id, response=stored_requests[req_id], timeout=False)\n",
    "\n",
    "\n",
    "# Run Flask App\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(debug=True, use_reloader=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('facadenv38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a56ea3102dc517391bfaea1ec803276257b008a2e9912ba73425789891105fa9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
